2025-FALL / ECSE-324-001
________________________________________
Captions
Search current recording by keyword
Brett Meyer, Prof: Okay. Alright… You have a good weekend? How much silk song did you play? Sadly, not a lot. My daughter has over 20 hours in the game already, which is amazing. I've got about 1… There will be more time later on in the semester, I'm sure, for video games. Or maybe not. Okay, let's jump right back into it. So I went back a couple slides just to set context, because, you know, maybe you don't remember what we were talking about on Wednesday. We had gotten to the point of talking about How computers do their thing. Computers are just big, finite state machines with lots of combinational logic that gets driven by it. You'll see just how much combinational logic when you do Lab 1. In Lab 1, you are going to build a computer. Basically, in its entirety. We will give you assembly language programs. You could write your own, too, to test things, and you will run them in a computer that you design from Gates. As such, as finite state machines with combination of logic, they have a clock. But in addition to finite state machines that you're used to, They also have memory. We cannot make a finite state machine sufficiently complex enough for it to save all of the state of the computer. So we have a separate thing called the memory that saves the state of the computer. Inside the memory goes… the program… And all of the data associated with it. The program, again, is written in machine language. You write assembly language instructions, the assembler turns it into ones and zeros. You specify variables in C or Java, X equals 3, That's not an instruction! But that's data, that's part of your program. The instructions to do manipulations on X, the values associated with X, they all go in the memory, they're all data. We have our high-level C code at the top, swapping two memory locations. It turns into what we have in the middle. That's, not ARM version 7, which is what you'll be learning, but you see the LD and the ST, you'll be dealing with things that load and store as well when you start to write programs. And it all becomes ones and zeros, all the data, all the instructions, and that all gets saved in memory. memory, the repository of computer states, because we cannot make finite state machines complex enough to do it, or general enough. Your computer Can have arbitrary states, because you can write arbitrary programs, but the finite state machine does not change from one program to the next. The finite state machine basically just looks in the memory for the next instructions, does what it's supposed to do, and moves on. That's what the finite state machine is designed to do. As you'll see, when you build your simple computer for Lab 1, There's just 5 states. Just 5 in your finite state machine. Everything else, all the other state is saved in a combination of registers. Temporary memory locations, and then the memory itself. Details that we'll get to. Okay. So that brings us to this gentleman. This is Von Neumann. Physicist by training. But one of the… Foundational thinkers of modern computer architecture, We will talk in this class About two different kinds of computer architecture. There's the Harvard architecture, unfortunately not the McGill architecture. Missed opportunity, McGill. And the von Neumann architecture, Von Neumann… like I've been saying. regarded everything as data. Instructions are data, variables are data, all data is data, all data goes in the memory, so that means instructions and data all go in the same memory. And every single operation goes to that one memory for an instruction, and then goes to that one memory for all the data that needs to be operated on by that instruction. The Harvard architecture said, well, it's actually better from a performance perspective if we can have instructions in one place. And data in another. Because if we have instructions in one place, and data in another, then we can go look for them at the same time. That allows us to do things faster. And what's fascinating… Surely unimaginable in the 40s. When von Neumann was building computers to simulate The consequences of different dynamite charge shapes in the starting of a fission reaction in a nuclear warhead. That's what he was about. That is the foundation of our computer architecture today. While von Neumann architecture is what every software program sees. Computer hardware? is Harvard Architecture. This might seem like a contradiction. But software's view is von Neumann. Hardware's construction is Harvard. And we'll see exactly how that works. Later on in the semester. But, when we talk about computer architecture, we are often talking about that precise interface between computer software and its view, the operations that a program can perform on a computer, on a particular computer design, and the implementation of those… of those features. There are lots of different ways to implement the operations that a Instruction set architecture exposes to a compiler, And so. The Harvard architecture is actually invisible to software. All the computer sees is von Neumann. But underneath the hood, we've got the performance of a hardware architecture, separating instructions and memories that we can access both At the same time, if we want to. For Lab 1, you're gonna build something with Harvard architecture. Which… Brings us to… Our first look at ChatGPT today! So, I went to GPT-4, again, this was over the summer, 5 wasn't out yet. And I said, what's the difference between Harvard and von Neumann architecture, and which is more commonly used today? GPT-4 came back and said… Maybe? Ugh. There we go. And actually, it said too much, so I asked it to restate what it said more succinctly. I couldn't fit it on a slide. It correctly says that hardware architecture uses separate memory for instructions and data. It correctly says that von Neumann architecture shares memory for instructions and data. Everything else that it says is basically wrong. It says that… Harvard architecture is common in embedded systems and DSPs. And von Neumann architecture is widely used in general-purpose computing. And… this isn't right. Because all but the simplest GPUs use modified hardware. Modified, sorry, Harvard Architecture in Hardware, and von Neumann Architecture for their software. I wondered to myself if it would do a better job if I asked it to just briefly state it from the beginning, so we tried again. Briefly, what's the difference? I have to say briefly, in order to get it to fit on the slide, so everything I ask After that point, I usually say that, otherwise I'm parsing through just pages of Nonsense, honestly. And this is what I got. Again, correct about the nature of Harvard architecture and von Neumann architecture. And this time, it says, again, von Neumann architecture is more commonly used in general-purpose computers due to its simpler and more cost-effective design, which was true in the 70s. If we're talking about the 70s, von Neumann was simpler then. And then Harvard architecture is used in embedded systems, and microcontrollers, and DSPs, where performance and efficiency are crucial, which is interesting to me, because… PCs and servers aren't performance sensitive. So, it gets the… Fact of the relationship between the two, correct? But GPT has no idea what it's talking about in terms of what is common in computer system design today. We'll come back to GPT again in a little bit. And if you missed it when I said it before, I would reiterate. Don't ask GPT a question that you don't already know the answer to. Okay. We now know enough to start talking about some of these building blocks in a little bit more detail. So here is a real rough diagram of a computer. Over on the right, we have the processor, which is control, or finite state machine, and a data path, where the actual operations are conducted. Remember that our finite state machine here is relatively simple. All things considered. five states in yours for Lab 1, and the data path is where, if you have an add instruction, that's where the add is done. If you need to load something from memory, it's the data path that figures out where. Memory is over there, at the top. And for the processor to talk to memory, there's a interconnection network. There's an interface from the processor to the memory and to other things. Typically, this takes the form of a bus. Do you talk about buses in 222? Tri-state buffers, Lucy says, boo. We'll talk about it a little bit in, in the class today. And, and later on. Suffice it to say, there's some logic that sits between the processor and everything else. That helps to facilitate communication. And then we also have, external input and output devices. They also talk to the processor. Through the interconnection Network, We'll talk about how that works in a fair amount of detail. Well, first, we're gonna talk about memory. So, you have all made use of computer memory. Every single time you declare a variable, every single time you write a line of code, you're declaring something that's going to go into memory. These are just a variety of examples of simple declarations that we might have. It's memory's job to keep track of your variables, and the compiler or the assembler helps with that by deciding where different things will go in memory. Different kinds of variables take up different amounts of space, therefore taking up different numbers of locations in memory. If you declare an integer. In our example here, that's 4 bytes, that's the case for ARM version 7, but for modern programming languages. Modern assemblers on high-performance hardware, they tend to be how many bytes? How many bits? We don't just do 32-bit computing anymore, what are we doing instead? Oftentimes. 64. So, 8 bytes per integer. We're gonna have, half words. At 2 bytes, characters, or bytes at 1 byte. And you can have arrays, which are… Sort of an arbitrary number of these things all strung together next to each other in memory. You can think about memory as… Like, mailbox? System. Everyone in the building associated with this mailbox has a different address. Memory is organized the same way. We start with zero. The address is, like, the offset from the beginning that you need to add to get to the thing that you want. So all of our addresses always start at zero. We don't typically have base 10 numbers of addresses, usually they're base 2, so we would go to 8 or 216 instead of 29. And the address specifies the location, and then in that location, we have data. It could be an instruction, it could be… a variable… Like we have here. But the… From the software's perspective. It is a linear array. If you want to look at all the memory, you can take an address and just start adding one to it, and you can read every single location. In high-performance computing systems today, that would take a very long time. There's a lot of memory locations. If you have a gigabyte of memory, that's over a billion different byte locations to take a look at. The number… Of locations that we have supported by our computer system is called the Address size? This defines the address space for 32-bit computers. We have… 4 giga addresses? Over $4 billion, not exactly 4 billion, because we're talking base 2, more than 4 billion. It used to be that 4GA was the maximum that you could have, because 32 bits were the most that we could use to address memory. With 64 bits, you can address way more memory than we will ever need in our lifetime. 2 to the 34 is a ridiculously large number. I don't even know how to begin to think about that. Elon Musk does not yet have that much money. We only need a few more bytes, a few more bits of address than 32 at the moment. So, if we have a 32-bit address space. We can figure out that that's 4 billion-ish addresses. How many for a 24-bit address space? Yes, and what's your name? Natasha. It's… 60 megabytes? Approximately 16. Let's figure it out. How'd you do it? That's right. So we have 2 to the 20. So 2 to the 10 is 1K. 2 to the 20 is 1 mega, so we have 1 mega times 2 to the 4, 2 to the 4 is 16, 16 megabytes. You'll want to get proficient with this. So, when we talk about the size of our memory, we often refer to how big it is in terms of bytes. That tends to be the smallest piece that we can care about, with a caveat. Especially in the space of machine learning, sometimes we actually would rather deal with 4 bits at a time. Someone cleverly called that a nibble. As opposed to a bite. One case where engineers did not let us down. I don't know what you do with 2 bits at that point, but, in general. the amount of data that is accessed by a computer is related to the size of the data path. If you have a 32-bit computer, it tends to like to access 32-bit variables, 32-bit instructions. 64-bit computers do the same thing. We call that the word length for a particular computer. 32-bit computers have 4-bit words. An integer… sorry, 4 byte words. An integer is 4 bytes. An instruction is 4 bytes. Same thing for 64-bit computers, 8-byte words, 8-byte instructions. It tends to be the easiest amount of memory to get from memory, and to send to memory, and so on. You can do other things. You can just write or read one byte, or write or read a half word, in this case two bytes. Sometimes that ends up being a more complex operation. Let's see what can happen. So, if we have… data. in our addresses over on the left, with the smallest address at the top, the largest address at the bottom, there are 16 of them, it goes all the way up to F. And then we have data stored in all those locations. Each hexadecimal value there is 8 bits? 8 bits per memory location. If I want to read… the value… the word… Stored at address 4, what I get… Is the bytes at address 4, and then the 3 that come next. That's by definition. We could decide to do it differently, but we don't. We do it this way because if you want to access what's at 0, you get 0, 1, 2, 3. You want to access what's at 4, you get 4, 5, 6, 7. And so on. So we've… if we try to read an integer out of that memory location. Boy, we have a problem now. What's the number that we've saved there? Because we have… PEX90 in location 4. Is that the most significant byte? Or is it the least significant bites? It's a reasonable question. The question that divided computer architects for a few decades. Not so much anymore. Depends on byte ordering. So, we have this word, Endian-ness. There's two options for determining this byte ordering. We can either say that the… The value that's in the smallest location is actually the largest part of the number. That's called Big Endian, where when we read 90124APE, we mean 90 is the largest part of my number, and EE is the smallest part of my number. Because we write these numbers left to right, just like you do in decimal, with the most significant digits on the left, and then on the right. Or we could do it the other way. Whereas 90 saves the least significant… or 04 saves the least significant byte, 9-0. That's called little NDN, where we have 90124FEE in increasing size. If I have a program that has been compiled for Big Endian, it will not work on a little Indian computer. Because the numbers are stored in the wrong way. Those two numbers are not equivalents. Big Nian versus little NDN determines how the data goes into the memory, how it comes out of the memory, and therefore the value that's represented in the computer. It used to be that this was a much bigger deal than it is today. The fact of the matter is that the world is basically Little Endian now. The reason has to do with the way that we do computer arithmetic. So… We perform our memory access, especially for getting really large numbers, for really slow computers. We are gonna get the least significant bite first. And we actually need the least significant byte first for our math. Because information propagates from the least significant byte to the most significant byte. You made a… a carry ripple and a carry look-ahead adder for 222, right? The critical path… thank you. Through that adder is from the least significant bit position to the most. To do computer arithmetic, we want the least significant position first, Information follows on from there. There is a caveat. that, ARM technically will allow you to change NDN-ness, should you so desire. The reason why this can be a thing is that, still back in, like, the 80s, 90s, and maybe early 2000s, I don't know, if you were gonna buy a sensor. that was designed, manufactured sometime earlier, where this division still existed, the sensor might produce numbers in big Andean fashion, and so you needed to be sure you could handle it, or write your software so that it was going to transpose things around. But the world is basically a little Indian now, especially now that ARM has essentially taken over Lucy. So, when we're starting our read at address 4, how do we know how much to read? So, we will talk about this… Maybe later this lecture, I don't remember, but… the computer executes instructions, and the instruction has to fully specify the operation that the computer is going to do. So, there's a different instruction for loading a word than there is for loading a half word, than there is for loading a byte. So the computer instruction says, give me a word, which means give me 4 bytes, or give me a byte, which means give me 1 byte. There isn't ambiguity there. Does that answer your question? Fabulous. Yes, Muhammad. by the global meeting scored. Yeah, I think, so… In general, when you're writing instructions, if you don't specify that you're not reading a word, then what you're going to get is a word. Because the word is the default size of a transaction with memory. Yes, and your name? Harring? ARDA. How many bytes is this memory system? We have 16 bytes saved here. 16 locations, 16 bytes. 2 to the 4 memory locations. Well, so, in this particular case, I'm only showing you 16. So, maybe it's 16 out of… 4 billion, but the… just because… ARM processors have… 32-bit addresses. Doesn't mean that it actually implements 4GB of memory. And so there's this question of how big can the address space be? That's determined by the size of the number that we can use to represent an address. And then there's the other question of how much memory do I actually have? And that's a hardware design question. Does that answer your question? Okay. Okay. In general. You know, we've got these different operations that can get different amounts of memory, get different amounts of data from memory. We often access words, Oftentimes, computer systems will say that you can only do aligned accesses. Which means that you basically need to be able to perform address, mod, the size that you want, and get back a zero. So, word locations. without loss of generality, are at 0, 4, 8, 12, 16, so on and so on. F-words can be accessed at even numbers, bytes can be accessed at any address. ARM does allow you to perform unaligned accesses, meaning I could say, give me a word starting at address 1. And if I do that, then in this case, I would get 4B45C490 would be my answer. From a software perspective, maybe that makes sense sometimes. From a hardware perspective, it tends to be slower. Which And actually, you'll never observe that Here, but if you take 444, unaligned accesses can be slower and slow down your program. Questions about memory? We will have an entire… Chunk of lecture notes on memory systems later on in the semester. Yes, santiago? Yep. And every address is a byte, yes. No? What is… what do you mean? Yes, okay, the name of the address when I'm accessing a word. And I'm actually… if I'm performing aligned axes, then my address would be 0, and it gives me the byte at 0123. the value in each byte is 8 bits. So each memory location, specified by one of our addresses is just 8 bits. But I don't want only 8 bits most of the time, and so the memory system has to have a way of me specifying an address. For a collection of bytes, We give it the least significant address of the ones that we want. If I want 4 bytes, Starting from zero, I just tell it, load from zero. And then it gives me the byte at 0, 1, 2, and 3, packaged up nice and neat so I can do the work on it that I want to do. Does that answer your question? Muhammad. Do we ever need to read or write to a single bit? ARM has made this possible with instructions that we will not talk about in this class. If the… If the person that's teaching 444 teaches it the way that I have taught it in the past, then you would learn about special operations that Arm does to allow you to go in and flip individual bits within a word. They create a special memory hierarchy that enables this. It can be very useful if you want, if you have, like, binary variables or flags that you want to set or unset, but in general, this is not a thing that we can do easily. If I want to manipulate a single bit, the way that I have to do that… with typical computer hardware is I have to read the entire word, or bytes, or whatever. Perform logical operations to manipulate a single bit in that, and then write it back. Read a byte, change a bit, write a byte. For instance. Does that answer your question? Okay. Memory. Collection of addresses. Each address holds a bite. Let's talk about the compu- the, the processor now. The processor interacts with the memory through the processor memory interface. I want to… So, our previous picture had an interconnection network between the processor and the memory. But I like this picture for one reason. The entire world… as far as the CPU is concerned, is just memory. Nothing exists except for memory. If you have a motor, you have a microphone, you have a speaker, you have a screen, it's memory. Microphone is memory that we read from to get the data there. A speaker is memory that we write to. To make sound. Motor is a memory that we write to, to actuate something. It's all just memory. That's the only thing that the CPU knows about. In deciding how to manipulate that memory to manipulate the world. Again, we have control, your finite state machine, the sequential logic that orders events appropriately to achieve the goals of the software running on it. And then we have the data path, which is the combinational part that actually does the data processing. Data processing can mean calculating addresses to figure out where data is supposed to come from in memory. It can also mean performing manipulations on that data once we have it. Change the color of a pixel. Change the volume of your output at the speaker. Change how fast your motor is going. This looks simple. This is not the system that you will be designing for Lab 1, because it is a cartoon version of a processor, but we've got the basic pieces here. Control, again, is a finite state machine. The data path, you can think of as being principally two things. One is called the register file, and the other is called the arithmetic and logic unit. The arithmetic and logic unit is where most of the work happens. It's a tiny piece of your processor that does all of the math that makes all of your programs do their things. The register file is… Where we save temporary variables. that we are manipulating in the ALU. We talked last time about how memory tends to be slow relative to processors. Memory is designed for density, not speed. And so we don't want to go to memory every single time we want to manipulate data. It would take too long. So instead, we read things from memory, save them in the register file, manipulate them in the ALU, save the results back in the register file, and then push things back out to memory when we're done. This is really important, because memory is about 100 times slower than the operations in the data path. We would love to be able to do 100 operations in the data path for every time we go to memory. That's not… Practical in general, we will look later on in the semester at how we get around that problem. Okay. What I just described is called a load store architecture. Where we load data from memory. Load is an instruction that is also in memory, so we read from memory our load instruction, we execute our load instruction, read from memory to get our data, do data processing. We're loading the data processing instructions from memory 2, we load a store instruction from memory, and then that allows us to put our data back into memory. This is not the only way to make a computer. This is a consequence of modern computer architecture. When we talk about, Risk… computers, What does ARM stand for? ARM is an acronym. It wasn't someone cleverly thinking, like, oh, it's my ARM. Is it Dragos? Dragos. Acorn Resolution, but not supposed to be H. Yes. The R in ARM is risk. Which means reduced instruction set computer. as opposed to CISC, Complex instruction set computer. Do any of you have SISC machines in the room with us today? Do you have an Intel processor? Yeah. This is… This was born of a time when memory was extremely limited, and we did not have compilers. We wrote all code in machine language, and so, in order to save how much memory we had. I might want to have an instruction that takes the square root of a value saved somewhere in memory. And so then to perform that operation, I would have to load from memory, then run a complex algorithm that actually does square root. Intel still compiles its code this way, but under the hood, it's a risk. The risk machine, because risk is the key to unlocking performance. We don't have instructions that operate on memory locations anymore. No data processing instructions operate on memory locations, because everything is faster if I have a sequence of loads from memory, a whole bunch of data processing operations, and then a sequence of writes to memory. I can make every part of every instruction look about the same. And then I can make it go faster and faster and faster as my transistors get faster. Disk machines have incredibly complicated finite state machines. Because the operations can be so complicated, like an add versus a square root, there's no comparison there. But in an advanced risk machine, we have a square root function, not a square root instruction. You compile your square root instruction into a long sequence of simple operations that load from memory, perform data processing, store back to memory. We will be talking mostly about Risk machines after this slide. But again, Fisk versus risk is kind of like von Neumann versus Harvard. Intel still compiles, with support for CISC operations, from the 70s. But under the hood, it's all risk anyway. They basically have a hardware step that takes complex instructions and turns them into a sequence of simple ones. So that it looks more like ARM. Who ate Intel's lunch? Arm did. Over and over and over again. Questions? So we've been talking abstractly, About operations and stuff like this. We're gonna take a look more specifically at The sorts of things that a risk machine can do. But before we do that, we need to define some terminology. Registered transfer notation is the language of instruction set architectures. And, If you go and look at datasheets for computers, you will still find that operations, like instructions, are defined in this way. They tend to describe operations on registers, indicated with R followed by a number. Remember that our simple computer here has a register file, a collection of locations that are… can be worked on by the ALU. They are numbered, sometimes named. We also sometimes have… special purpose registers that have names different from R, like the program counter. The program counter is what saves which instruction… in which memory location am I currently executing an instruction from? Because that can help me to know which is the next instruction I should be executing for memory. IR is for the instruction register. That's where the instruction that I'm executing from memory sits in the processor so I can look at it and figure out what I should be doing in order to implement its functionality. We use mem with brackets around A to indicate the memory array. A is an address. If I want to perform a load. From address 4, A is just 4, and I get the data out of memory. And then we have an arrow to indicate the movement of information, and then… If we have things like add or logical operations, then we have notation similar to what you would use in VHDL, in your 222 experience, to describe the manipulation of the contents of registers, which are really just a collection of bits. I don't think I said this yet, but if I have a 32-bit processor. My register file, each register is 32 bits wide. So if I perform addition, I'm performing 32-bit addition. Logical operations, 32-bit logical operations. And so on. So let's see some examples. This is not ARM version 7 assembly, this is generic assembly. Arm version 7 would say LDR instead of load, but we will start talking about that in a couple of weeks, after you have designed your own computer. Our memory instructions? Put simply, we can load and we can store. Those are the only things that are allowed to access data in memory, because we have a risk machine, not a sysk machine. For load, we specify the address that we want to read from, and the register that the data is supposed to go into. So, loading from a particular address, like address 4, Like this, would get a 4-byte word and stick it into register 2. It goes into register 2, and then I can do operations on it in the ALU. Door does the opposite. It takes the data in register 4 in this example, and saves it into the location in memory defined by address. Address 4. We write that like you see at the bottom. 4 goes into… The particular memory location. Low store architectures, I'll start by reading from memory. Doing some work. And then writing to memory. The work is done in the ALU, the arithmetic and Logic Unit. For instance, we can add, we can multiply, we can subtract, we can shift, we can perform AND, OR, X, OR, And so on. Add, as specified this way, takes R2 and R3, Takes the sum, and saves it, and register 4. I should have this picture on all of these slides. So we have… We load a value into R3. We load a value into R4. We execute, add, R3, R4, R… and it goes into R2, All that data is now in the register file, and then we can store R2 back into memory. For instance, That's the typical workflow of a risk machine. Muhammad. When we perform a load operation, the previous content of R2 goes away. It's overwritten. So if you cared about it. You needed to save it to memory before you performed the load. Compilers… Compilers make promises about correctness of program execution, and so if the thing that was in R2 before this operation is performed mattered, the compiler would make sure that it's saved someplace. You will not benefit from a compiler this semester. The compiler is you. So if the thing that's in R2 before you load into it matters, you have to put it someplace. Lucy. This place will get it. Nothing good. Bye. Correct. If you misplace or forget an address or a variable, there is nothing that can be done to recover it. There might be some… So, what's interesting, Lucy, is that the counterexamples to this are all the ways that clever people have figured out how to hack architectures like, Heartbleed and other things like that, sort of, like, looking at Data on the stack that was not… that has not yet been overwritten, and you can still Read that stuff, because it hasn't been overwritten yet. But in general. remember that your computer is just storing voltages across capacitors. You drain a voltage, or you charge a voltage. It has no memory of what its state was before. Because it's just a capacitor. There might be people that disagree, but there's no life in a capacitor. Hey, Phil! Here's a simple program. I can write it in C, or whatever. I define three variables, I want to perform A… an addition operation. I want to… I define X as 1, Y is 3, Z is 7. I perform X plus Y, goes into Z. At the end of this execution, Z is 4. The fact that it was 7 before has been forgotten. And our simple program. loads from address X into R2, loads from address Y into R3, performs R2 plus R3 into R4, and then stores R4 into address Z. Simple enough. Yes, what's your name? Say that again? Ryan. Where's the part where we assign the value to the actual variable? So we will talk about this in more detail when we talk to… talk about system software. If you've ever… Compiled, like, a one-line program. And then you go and look, and like, oh, my one-line program is 300 kilobytes. Why is that? Certainly, I didn't write enough information to take up 300 kilobytes, 300 kilobytes? That's like a long essay, if you write it in a text editor. Every time you compile a program. Stuff beyond what you write also gets put in there, and part of that is initialization code. So when you hit compile, the compiler copies and pastes in program before yours that takes all of those initialization values and saves them in locations that the compiler has decided for you. So it basically has a long list of memory locations in your program, and a starting point, and locations in computer memory, and it goes, copy, copy, copy, copy, copy, copy, like that. And that's how initialization happens. And it used to be that if you didn't initialize variables yourself. The program would just give you whatever was in that memory location before. Which is why, if you use older compilers, you can still get warnings about using an uninitialized variable. It's much more common today that everything just gets initialized to zero unless you specify an alternative, and then you sort of, like, copy zeros into memory for those particular things. Does that answer your question? Yes, and name? Collar. Those are equivalent, yes. So, that happens before your program executes. We won't talk, we're not going to look specifically at the initialization code in this class. But… Before your program starts at Maine, Lots of stuff has happened. Part of the stuff that happens is copying the value 1 into the address for variable X. And the compiler figures out where all the variables are supposed to be, and the compiler generates the code that performs that initialization for you, and makes sure that that code runs before your code runs. If you put a thing at main, at the first line of main, it is not the first thing that the program executes. It's… Yeah, it's initialization. It's invisible to you, the software programmer, but the computer executes it. Yes, and your name? Ryan. Yes. So, how am I changing my C program? X equals Z plus Y. So, and then your question is, when do I… do my stores. When you do your stores, depends on what compiler optimizations you have running. So, the way that compilers always get it right. is that they will do the most stupid word-for-word translation of C code into assembly, even if it means repeating stuff and making things slow. And then a series of optimization passes will happen that will look for things that only need to happen once and have multiple effects, or they'll change the order of things in order to make it more efficient. In this class, where you are the compiler. For your first pass through the assignments for Lab 2, a simple word-for-word translation will be great. And then, most likely, I'm going to ask you to go and try to make it go faster by changing the order of things relative to the C code. But if I put X equals Z plus Y, then… It's… It can be equivalent to have load, load, add, store, add, store. But it's also, from the program perspective, the same to do load, load, add, add, store, store. Doesn't matter. The correctness of a program is determined by the correctness of the state of memory. At the end of execution. And if you take 425, ECSE 425, that I'm teaching next semester, then you'll see all the different ways that modern computer, modern CPUs change software dynamically, on the fly, while it's running, in order to make it run faster. Muhammad. Goodbye. All sports will contain… After the last store instruction, does R4 still contain the value there? Yes. Door is non-destructive. It's basically copying from the register file into memory. Again, the register file just has capacitors. Reading from it doesn't damage what's there. See you later! I am working on learning all of your names. I have them from Minerva, but there's 155 of you, so that's really hard. But my goal is to be able to greet you by name as you walk out of my class. And if you have to be someplace, you have to be someplace, but… Yes, what's your name? Maeve, yes. When we store from R4 into memory, we are just performing a copy. The value that's in R4 is copied to main memory. We can continue to use it in R4 should we need to. The register file, Is made out of transistors, and it has a fixed size. So, we can never have more or less registers than what is defined when the hardware is made. For the purposes of this class, that is absolutely a true statement. Advanced computer architecture makes a mess of a lot of the assumptions that we make about simple computers, but for this class. You've got exactly 16 registers. Always and forever. And if you need more than that, you can't have them. You have to make space in your register file by saving stuff to memory temporarily, and then loading it back later. And the reason why CPUs tend to have relatively few locations in the register file is for performance. It's an array of memory. The delay required to access an array of memory is defined by its width and height, Because of capacitance. And so… Typical of the ARM universe is 16 or 32 registers, because if I make it bigger than that, it starts to slow other things down. And, compilers are pretty good See you in a bit, Shotty. Compilers are pretty good about making good use of registers. But if you have lots and lots of variables that you need to be working on all at the same time, it will slow you down because you'll have to save some of them in memory. Does that answer your question, Maeve? Okay. Alright, so… Software looks like von Neumann, hardware looks like Harvard. In von Neumann's view of the world. instructions and data are in the same memory. So we have our simple program there, starting at address 0, and then at memory locations down Beyond our program, we have our variables. So this initial… this initialization Program that runs before your program runs. It copies 1 into X, and 3 into Y, and 7 into Z at locations defined in your program. And then the program runs, starting at address 0. Okay. So we have a slightly more complicated picture of our computer now. We've taken our simple one with the register file ALU in memory, and we've added a couple things to it. Remember, instructions are data. I have to read instructions from memory in order to know what the CPU is supposed to do. So where do I put it? Every time I get an instruction from memory, I fetch it from memory, it goes into the instruction register. the instruction encodes in binary the operation that the CPU is supposed to do, and so then the control unit, which is not pictured here, can look at different bit positions in the instruction register to know, this is a load instruction, or this is an add instruction, this is a store instruction. They get encoded in different ways. The program counter… It is the address of the instruction that I'm reading. It's my location and memory. So at the start of this program, the program counter is equal to zero, because that is the address that I'm going to use to read the first load instruction from memory. The CPU reads from address zero, it gets that load instruction back, the load instruction goes into the instruction register, and that information informs the finite state machine that controls the ALU, controls the register file. And ensures that the operation occurs as defined. We call that process of figuring out what it's supposed to do, decoding. The instruction is encoded in an instruction. In the 32-bit instruction register. We decode it to figure out what the operation is supposed to be, taking those 32 bits and turning some of them into control signals from the finite state machine to the rest of the combinational logic. The execution of the instruction happens after it's been decoded, when all the control signals have been figured out and stabilized. And then at the… when we're done… Executing the instruction. We update the program counter, We make it point to the next location. We execute instructions sequentially, one after the next. Until we have an if-else or a for loop, or something, and then we need to be more clever about it. Fair. Just a second, let's see. This is… How far do we have to go? Oh, we're almost done! I'm supposed to finish this set of lecture notes today, and just wanted to make sure that that was still possible. Alright, so we have… this is our… still our simple program. There's one additional bit of information that's on the screen here now. when we compile our simple program in ARM version 7, it gives us… we get out machine code… where's my laser pointer? There, I can barely see it. This word here. That is the machine code. for load, from that address into R2. And I want to point something out to you. You see the 3-0 in the least significant byte of that instruction there? Great, if I can get my finger off of it. Nope. You see the 3-0 here? Let's go backwards. What address are we getting X from? 3-0. So parts, at least, of the address, we can't specify a full address in an instruction, we'll talk about that more later, but part of the address, part of the address calculation, is actually in the instruction. what we're actually doing here is we're saying, I want the thing that's in address 0 plus 3.0. That's where X is. And you see the number 2 right next to it, right next to the 3-0? Register 2 is the destination. The first character is E59, the first one is telling the ARM processor Whether or not to always execute this instruction, we'll talk about that more later. The 5.9 specified that it's a load. We want to always load from 3-0 into register 2. That's what the instruction encodes there. And then the other ones are similar. We have our load, load, ad store. If we begin to do the execution process, how does this work in our simple imaginary processor that fetches an instruction from memory, decodes it to figure out what's going on, executes it to actually do the thing, and then updates the state of the register file? Step 1. We reset our computer, PC is zero. That's the first location that we were supposed to read. So we fetched! We have our register transfer notation. The instruction register is updated with the data in mem, indexed by PC. PC is 0, that's the first instruction. We decode it. It's a load instruction. We execute it, That allows us to update R2 from the location in memory. we get X's initial value, 1. We update PC so that we can continue to move on. PC is now 4. We fetch. We decode. We execute. Our second load has been performed. Update PC. We fetch, we decode, we execute. Now we have R4, That is the Z variable. Initialized to the value 4. Update PC. Next, fetch. Decode. Sorry. We don't have to load Z. This one that we fetch is actually the add instruction. We fetch from memory. We decode, see that it's an ad, we execute it, and that changes the value of R4. It was… did not have a value before, or it was… Something else? And then, finally, we take the value that's in R4, and we store it back into memory. The hallmark of a risk processor is the systematic fetch, decode, execute, fetch, decode, execute, fetch, decode, execute. We can make this go really fast. Whereas with a sysk processor, it's not the same set of steps. Because not all instructions have the same complexity. I take a square root of a memory location is very different than add this register to the next. Square root of a memory location takes a lot more operations than add this register to the next. But we are able to have your 4GHz processor or whatever, because each of the steps has been subdivided Beyond even these 3 here, Sometimes 20 steps instead of 3. Other times, 10 or 15. And that allows us to make it go very, very fast. That's the advantage of risk over CISC. So, there's one thing that the program hasn't done, and that's control flow. Our simple program just runs from top to bottom, always the same every single time. But your first program probably included an if-else, unless you… your first program was literally Hello World. If it was literally Hello World, it was more like that. But if not, then you might have had some different branching that's possible based on the value of input data. If we have a condition on our variable X, then it means that in one case, we want to execute one instruction, and in another case, we want to execute a different one. We have to have some way of doing more than just PC equals PC plus 4. Because if I'm always saying PC equals PC plus 4, I'm just walking through my memory, one location after the next, and doing all of the instructions. So what we need, actually, Is an instruction that modifies the program counter. So if we have this simple program here, there's our C code. It doesn't look like much, but it becomes all of that assembly. The first thing that we have to do is check the value of X. Checking the value of X is the only way we know which other code we are supposed to execute. So we load X into R1, And we want to compare X with 0, so we move 0 into R2, Now we have a branch instruction, BLE stands for branch less than or equal to. Less than or equal to is the opposite of our condition up there. Our condition is greater than… If it's greater than, Y equals 7. If it's less than or equal to, Y equals 13, yes, and your name? Lenny? So… Load… Gets a location, gets the data at a location in memory, and puts it in a register, requires a memory access. It works no matter what the data is. If I know for sure that the number that I want in a register is zero. I don't have to access memory. I can save the zero inside of the move instruction. And I just copy that zero from inside the move instruction into the register. You use move for constants. That's right. Small constants in general, because my instruction is 32 bits. I cannot assign a 32-bit number using a 32-bit instruction. I need some of my instruction to actually specify what it is. So, in ARM, you can do… 12-bit? Moves… In some cases, 16. But not in all cases, and they have clever tricks for allowing you to have larger numbers, but you can only have relatively small amounts of information that you make use of in a move instruction. If you want a 32-bit constant, we have to use a load. Does that answer your question? Fabulous. So the branch instruction, branch less than or equal to, is going to compare R1 and R2, and if R1 is less than or equal to R2, We change the program counter to 0x14. So, if the condition is true. Update the program counter with a new address for the next instruction. Now, what that means is that if R1 is greater than R2, R2 is 0. If X is greater than 0, we don't Execute the branch. The branch does not occur, and I execute the next instruction. The next instruction moves 7 into R2, just like our C code does. But, if R1 is less than or equal to 0, which we just put in R2, then we branch or jump over the instructions until we get to address 14. And at address 14, we move 13 into R2. Now, there's another instruction in here that I haven't talked about yet, and it's this one. It's the… I'm a little dissatisfied with my laser pointer here. You see this B here? Remember, BLE, branch, less than or equal. B, branch. Always. No condition. An unconditional brand. Why do we have that B at address 10? Yes. So you don't execute the else, that's right. We don't want to set Y equals 7, and then set it Y equals 13. That would not be a correct implementation of our software. So if we set Y equals 7, we have to jump over the code, That does the else part. you jump till after your pointy brace, or however it is. You jump to the next point that has the same level of indentation if you're in Python. And then at the end, just for the sake of completeness, we save our value of Y into memory. And that'll either be 13 or 7, depending upon what the value of X is. Yes, and your name? Ed, yes. So, why can't we just put zero in the BLE? The answer is that… You could! If your computer supports that, So, some computers support A branch equal to zero. And you specify one register, and it has a single comparison. Or a branch not equal to zero. Specify your one register, you have your comparison, the instruction set architecture? Defines what instructions are available to the compiler. And makes a… and then the hardware designers are making a promise to implement all the features that are described. So, branch less than or equal to, with two registers in the comparison, is… That's the most general way of doing it, but you don't have to do it that way. And in fact, the way that ARM does things is not like this at all. Every computer might make different choices about which of these things to make available. Some computers only do branch equal to zero. There's no other option. And then it's up to your software to turn all comparisons into a comparison with 0. If you want to compare something with 1, well, you subtract 1 from both sides, and you're doing a comparison with 0, for instance. Does that answer your question, Ed? No, you're going to use ARM version 7, which we will be talking about in Chapter 3 of the lecture notes. You will be breathing, weeping ARM version 7 by the end of the semester. A small fraction of you might come to love assembly language programming like I did when I took my equivalent course, but… My experience is that most of you won't. on version 7 from here until forever. Except for Lab 1, where it's a special customized one that Professor Dubak designed specifically for Lab 1. Muhammad. All of the instructions are stored in addresses that are 4 bytes apart. 2, 4, 8, 12, 16, 20, That's a hexadecimal number, remember? 1-0 is… 16, 1-4 is 20, 1-8 is 24. Okay. We're almost there. So, in… This version of the program, our branch instruction specifies the address that we want to go to if the condition is true. 0X14. Hexadecimal 14. As an assembly language programmer, it would really suck if you had to always be calculating out addresses in hex. Fortunately, you don't have to. You can instead put little labels over on the left-hand side of any assembly language location, and then you can refer to that label in your assembly language code. We can branch less than or equal. R1, R2, health. And then what the assembler does, is as it goes through your code, it knows that every instruction is 4 bytes apart, it knows what the memory address is for else, it can do that math for you, and then it substitutes the appropriate address into the instruction on your behalf. All you have to do is just name things properly and refer to them properly. What's the worst that could happen? Well, when in your program you've got, like, 20 labels, then it's easy to get them mixed up, so… Name things properly. And so, in this particular case. We have our two branch locations. 1-4 and 1-8. We can just use labels over on the left, and then the assembler will fill in the blanks for us accordingly, and we can refer to them by name. So we've looked at if-else, what about a loop? So, in a if-else, When we branch, we're moving forward to higher addresses, we're jumping over things. For a loop, however, we might need to sometimes jump backwards. So, our loop, again, some simple C code. We're just taking… We're just adding a number with its… Self? Over and over again. What's different about this chunk of code than the other one is that at the bottom, we have a B, an unconditional branch, back up to the top of our loop. You get to the end of a while loop, what happens next? You go back up to the top, and you check your condition, right? That's what you were taught when you're doing your intro programming. You check your condition, you do the loop body, you check your condition, do the loop body, and so on. And that's captured here in our assembly. The first thing that happens inside of the loop is we have a… we have some setup for our comparison, BGE, branch greater than or equal to. And if the condition greater than or equal to is true, then we jump to end. Otherwise, we execute our loop body, and then branch back up to the top. Where we once again… Perform our check, execute the loop Avi, maybe, or jump over it to the end. Bullet loops look the same. just slightly different… slightly different rearrangement of the manipulation of our variables. Yes, Ryan? We've had other events. So… You would have to do… Your labels would be outer loop and inner loop. Outer loop A, inner loop B, Filter loop, do it! Filter loop done! you're gonna drive yourself crazy with trying to figure out how to name locations in your programs as I give you things with triply nested loops and other crazy logic. But yes, does that answer your question? Is there naming conventions that go along with this? Unfortunately not. But the… The approach that I tend to take is… You know, you've programmed with functions. You should name all the code related to a function with the function name, and then all the subparts by adding additional stuff. And just be systematic. Technical debt accumulates so fast when you're programming an assembly. Comment your code, use sensible naming, we'll make your life easy, there's no shortcut to success. Other questions? Okay. Yeah, we'll just… It is time to wrap up and see if there's additional questions. Okay, so… The purpose? of Lecture 1, Was to give you a really rough overview of the scope of this class. We've set the stage with a little bit of history. We've talked a little bit about computer organization, introduced the load store architecture, the sorts of things that it can do. Everything for the rest of the semester is going to be looking at different parts of this in more detail. And at the end, you should be able to design a computer And write your own assembly language for it. That might seem like a daunting task, but you'll be there before you know it. I will see you on Wednesday.
Play Video
Seek back 10 secondsPlaySeek forward 30 seconds
Mute
Current Time 0:01
/
Duration 1:17:53
 
1x
Playback Rate
Captions
FullscreenPicture-in-Picture

