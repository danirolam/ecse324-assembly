
2025-FALL / ECSE-324-001
Captions
Search current recording by keyword
Brett Meyer, Prof: Okay, okay! So, last time, We finished up the processor design set of notes. Which should have told you everything that you need in order to finish Lab 1. Have you finished Lab 1? A handful of people have finished Lab 1. Today, we're going to dive into pipelining, which is really the reason why, when you did your processor for Lab 1, we divided it up into 5 clock cycle stages. The reason for that was not particularly well motivated. Before we'll get into that today. If you are Struggling with Lab 1. If you have ease in your output and you don't know why, if you're getting errors on different lines and you don't know why, it is possible to do cycle-by-cycle debugging of your processor. It is possible to go and look at the programs that are running on your processor and understand what each instruction is doing so that you can figure out by hand what the output ought to be and why. This is the recommended path. I spent… I'm happy to do that myself. A couple years ago, when I first solved Lab 1, I'm sorry that the instructions for how to debug cycle by cycle are not more clear in Lab 1. That's something we'll fix for the next set of students. But you can just click on the little clock button, and it'll advance Time, half a clock cycle, and you can see how signals are changing, and help you figure out how to make adjustments. Did you have a question, Lucy? It's not all about the height. A note about the pipes at the top. Yeah, when you… The what? Yeah, tunneling. Tunneling should also be mentioned way earlier in the… in the lab, yeah. I'll make a note about that. Again, for the next time I teach the class. Okay, so today, we've got a one-shot. We're going to get through this, hopefully, all in one go, and then next time, start talking about instruction set architecture. That's the assembly language programming, the software interface to hardware. We'll be putting hardware behind us. Mostly, though not entirely, after today. You will note at the end that the slides that I have might differ from what you've downloaded, because I found an inconsistency just before class. I will have the PDF on my courses updated after class today. The problem is obvious enough that you'll see it, and small enough that you can just correct it yourself if you're annotating anyway, but I'll have a clean PDF available on my courses. Okay, so here's our data path. We've been talking about 5 stages of execution, fetch, decode, execute, memory, and write back. You can see the four of them that apply to the data path labeled there with write. back. Annotated in a slightly different way than what was… what we saw last time, but it makes sense because Writeback sort of starts at the end there, and ends in the register file, where we update values. And we have, little dotted lines. Separating each of the different stages. And… You can see them, all the dotted lines, going through different temporary registers that we have, and they also go through the register file. We can think about the register file about as participating in two different stages. It participates in decode, where we read operands, and it participates in writeback, where we write results. In between that, The ALU does work in the execute stage. The memory stage is where we access memory, if we access memory. If we access data, I should say, Which reminds me, I don't know if I've said this enough times to prevent you from getting questions like this wrong on the midterm. How many times is memory accessed to execute an instruction? The answer is definitely not zero. How many times? There's two possibilities. There are two correct answers. Zero is not one of them. How many times is memory accessed over the course of executing an instruction? Muhammad. It's 2 in some cases. When do I need two memory accesses to execute an instruction? When the instruction is a… a load, or… Oh, restore. How about all other instructions? How many times do I access memory to execute them? Once! I must access memory at least once for every single instruction ever executed. Always and forever, I cannot execute an instruction without accessing memory. That's where instructions come from. They come from memory. He sees my address. Load and store instructions, I access memory again. using an address calculated in the ALU outputted by RZ into the processor memory interface. So if I know about how many load store instructions I have, and how many other kinds of instructions I have, I can figure out on average how many times memory is accessed per instruction. It's about 1.2 times per instruction. Because memory access instructions tend to… To be about 10-20% of all instructions. Just sort of depends on the workload. But it is not possible to execute an instruction without accessing memory. I like to ask questions about this on the midterm and final, just to see if you understand what memory is, how it's different from the register file, how it's different from storage. These words have important meanings in this class. And it should be easy points. I've discovered that there's no such thing as easy points in this class, but I'm trying to do you a favor here. Muhammad. When… the question is, do we differentiate between main memory and cache? And, we will answer that question when we start talking about cache. But cache, which we haven't introduced yet, is part of the memory hierarchy, and when I talk about how many times do I access memory. For the purposes today, and for most of the course, I mean from the CPU's perspective. How many times does the processor request at the processor memory interface something from memory? And we'll worry later about where exactly the something comes from. Okay, pipelining. If you have a better idea for an analogy here, I'd love to hear it at some point. The analogy that I think I was taught when I learned about pipelining In the late 1900s, Or maybe it was… It could have been 2000, I don't remember exactly when I had my computer architecture class. Laundry. We pipeline laundry. I don't know if you do, but at my house, we definitely do. There's 4 of us, including 2 teenagers. We make a lot of laundry. We could, for each load of laundry. Put it in the wash, then put it in the dryer, and then fold it. Before we start doing the next one, where we put it in the wash, put it in the dryer, and then fold it. If we did that, 2 loads of laundry would take 6 hours if every step Takes 1 hour. That is not an efficient way to do things. I imagine you can think of a more efficient way to do things. What you're imagining is pipelining, Rachel. Once I picked up my first… Yes, this is exactly what processors do. The moment we finish the first step of executing an instruction. We take the instruction out of the washer, and we put it in the dryer, and then we can put another instruction in the washer. Processors do this on a vast scale. There's 3 stages here. There's sort of a, at a minimum, 3 stages in a processor. Fetch, decode, execute. But oftentimes, processors will use many more stages. I think that the maximum number of stages was in the early 2000s in the Pentium 4. It was 20. 20 stages of execution for a single instruction. Every single instruction went through 20 stages of execution. You guys have a question? Are you good? Thanks. Okay. So, this is probably how you do it. Instead, Pipelining. If I pipeline, and I take the same amount of time, 6 hours, I can actually finish 4 loads instead of 2. Or complete 4 instructions instead of 2. And… while the first load might take 3 hours to finish, just like before, after that, I've got a new load done every hour. So… It's effectively… Taking just 1 hour to do a load of laundry. Because every single hour, I have another one completing. Instead of every 3 hours. Again, the beauty of pipelining That's why Intel could take 20 cycles to execute a single instruction. But if after that, you get a new instruction completing every cycle, it's like you're just taking one cycle. And then the whole push. in the late 1900s and early 2000s, was to make clock cycles shorter and shorter and shorter you might be adding more and more pipeline stages, but effectively, you're just making your instruction throughput higher and higher. Because once the pipeline fills, you get a new result every single clock cycle. This was how we unlocked massive performance With deep pipelines and fast clocks. There is no free lunch, though. Let's see… And we'll get to all the different things that can go wrong. But it is essentially… Doing an assembly line kind of instruction execution. And this is facilitated By the picture that we've already started drawing here. Where we have a uniform description of each different instruction based on these stages. Every single instruction goes through the same stages. You do the same exact processing on them all. They'll differ slightly in the different stages, but every instruction goes through the same stages. So if we want to draw it out, it looks like this. Cycle 1, we fetch an instruction. PC equals PC plus 4. In the next cycle, we begin to decode it, and at the same time, we fetch an instruction. PC equals PC plus 4 again, cycle 3. We are computing the first instruction, decoding the second instruction, and fetching a third instruction. Interesting things start to happen as we fill our pipeline. When the pipeline is full, we are fetching an instruction in every single cycle. Can you see a problem that starts to emerge when we do that? If I'm fetching, let's say I'm 100 instructions in, and I'm fetching every single cycle, what's the problem that starts to emerge? Yeah, and what's your name? Dustin. We will get to data hazards, absolutely, but let's just talk about the processor memory interface right now, William. Thank you. Branching definitely screws things up, that's called a control hazard. We've got data hazards and control hazards mentioned so far. The third kind that I'm pushing for at the moment is a structural hazard. What if… Do you have it? We definitely do, and we'll look at how that works, Derek. Lucy? What if we are… First command, right back to a section of the register file that we expect to be written off by having the second command. That's another data hazard, and we will look at how we make that happen. It's good that you're thinking about these sorts of things. But if I'm fetching every single cycle. What happens if one of the instructions I fetch is a load? So if I'm in the memory stage with my load, and I want to perform a load. but I'm fetching instruction at the same time, That's a structural hazard. Because I want to perform two memory accesses. But I only have one processor memory interface in this picture. So, you've done a great job of telling the story for most of the rest of the class today. Data hazards. When I need the output of an instruction to be the input of another instruction, but maybe it hasn't finished yet before I try to request it. Control hazards. If I'm gonna take a branch, and I've already started fetching a couple more instructions, And then, structural hazards. If there is a physical component somewhere in the system that two different instructions at different stages of execution want. And the most obvious example of that is the processor memory interface. Muhammad asked about caching earlier. Caching is one of the solutions to the structural hazard problem. You don't have to understand how that works right now, we will spend days talking about caching. All this to say, Lots of pipeline stages. up to one instruction finishing per cycle, but there's no free lunch. There's lots of things that can go wrong that will slow down my pipeline. And that's what we're gonna get… take a look at today. Okay. Doop, doot, doot. There we go. Data hazard, control hazard, structural hazard, contention for shared resources, like the memory interface. Alright. I'm just gonna flip to my picture. So here's a redrawing. of our processor diagram. Everything is still there. And Derek, tell me if this gets at the question that you're asking about. In our previous drawing, we had temporary registers like RZ that holds the output of the ALU, and RA and RB that hold the outputs of the register file. All of those temporary registers are now collected into these interstage buffers. between… see, do I have it? No, I don't. Okay. Between the decode stage, where we have the register file and the instruction decode logic, and the execute stage, I have interstage buffer B2. This subsumes RA and RB, but it also holds debugging informa- not debugging information, the decoding information that I need to tell the ALU what operation it's supposed to do, for instance. Because… If I flip back to… Our picture here. And if we think about your processor, you have control logic that specifies the ALU op. as a function of the instruction register, and it's just an input to the ALU, and it just comes from your control module. But if I'm executing 5 instructions at a time, there's 5 different ALU ops. I don't always know the one yet from Fetch, because I haven't decoded the instruction. And I don't really care what the ALU op is anymore in Stage 5, because it's already been done. But the point is that, just as information is flowing through my pipeline. Control signals need to flow through my pipeline, too. So I make a copy of the control for the later stages, and it passes through temporary registers. Just like information passes through RA, RB, RW, RZ, RD, And so on. So instead of having all those different named ones, we have interstage buffer B1, B2, B3, and B4, with our stages of execution there. It's right back. It's sort of… In the bottom there, and back at the top. It's in between buffer stage B4 and, I guess, B1. Any questions about this? Yes. We will definitely look at examples of, like, data hazards and stuff. That's what you're asking. We're going to spend the whole rest of the class looking at examples of assembly code and hazards. Okay. So… We have a pipeline. And we have some things that can go wrong. And when things go wrong in our pipeline, we end up needing to stall an instruction. Meaning, we make a later instruction wait. For an earlier instruction to complete. So that it has the correct information. Note that… It's the hardware's job To make sure that the software is executed correctly. So I should be able to take a program. And run it on a single cycle processor, where every instruction does all of its work in one cycle, or a 20-cycle processor, where every instruction takes 20 steps, and the output needs to be exactly the same. Because the software developer doesn't want to have to think about that, and they shouldn't have to. They should just be able to take their code, compile it, it executes, you get the same result wherever. This is kind of like how, if you're writing an app for the App Store. you want it to work on an iPhone 17, and a 16, and a 15, and a 14, and a 13, I don't know how back we're… how far back we're going as far as Apple support is concerned anymore, but I want new software to work on my iPhone 13. The architecture for the processors might be different, might have different memory hierarchies, might have different number of pipeline stages. Could have different low-level organization. But hardware must guarantee that everything works exactly the same. So hardware has to manage all of these different issues that can arise because the computer engineers decided to do pipelining. One other source of delay that we haven't talked about yet. Is the fact that memory might not… Be accessible in a single clock cycle. So, instructions want to advance through the pipeline one cycle at a time. But if it takes 2 clock cycles to access memory. then every single time I access memory, I have to wait. How often do I access memory when I'm executing instructions? At least once. Remember that. I don't remember if I asked that question on the midterm, I've already written it, but if I haven't, I will definitely ask it on the final. Yes, Lucy? You could bring up the midterm now, and we can take a look with you. You know, that's a really great idea, but I don't think we have time for that today. Yeah, Theo. When you… when it needs to weigh, do you just have lower performance, or… It does not cause a bug. But it just causes a delay. Yep, so you… the program just ends up taking longer to execute. And so… all of these different sources of stalling. Remember, we get high performance by having many pipeline stages in a short clock cycle, which means that if I were to make many pipeline stages, but then have a whole bunch of stalls. There's no point in having a 20-stage pipeline, and then having every instruction wait 100 cycles for a memory access to complete. So, the art and science of computer architecture, ECSE 425, is really about how do we organize computers in order to eliminate as many of these sources of delay as possible. With fancy memory hierarchy. With branch prediction. With all sorts of other fun stuff, too. Take 425. I'm teaching it next semester. It's full. But… I'll teach it again the following year, too. The first time I taught computer architecture here was in my second semester, so, like, 2012. There were 15 students in it. Right now, there's 120. So, there's just a lot more computer engineers in the department now than there were back then. But it's a fun class. I think it's my favorite class at the undergraduate level to teach. Anyway… Okay. Independencies. We have a data dependency here. Because our add instruction, Its result is saved in R2, And that result is used in the subtract instruction as one of the source inputs. So, we… for the subtract instruction, we're doing R2 minus R8, but R2 is calculated by the add. So we can't perform the subtract until the addition is done. That's a data dependency. It's a read-after-write data dependency. You don't have to remember that, but we have ways of labeling all of these things. Because we want to read it after it's been written, we can't proceed until we can read the output of the add instruction. So we cannot, unfortunately, organize our execution as illustrated here, because when I'm trying to do decode in Cycle 3 for the second instruction. I'm only just computing the result that I need. That's a problem. So, this is… Naively, what we have to do. This is not what actually happens, but if we don't fix this problem, this is what would happen. We would have to make the subtract instruction wait. It has to wait until Cycle 6, to read what was written in cycle 5 as the result of the add instruction. So we essentially have 1, 2, 3 cycles of waste there, 3 cycles of delay. Waiting for the add instruction to complete. That's not great. We're throwing away a lot of the advantage of pipelining if this is how we have to do it. Are you tracking with me? You see this problem. Because I'm going to show you a circuit-level solution to this. And spoiler alert, it adds multiplexers to our data path. Everyone loves multiplexers. Muhammad. The circuit implementation of the solution to this problem. Yes, and what's your name? Adam. Adam. Why? It's, it's right. Except flow cycle for the compute of the next… So… Let's think about clock edges. In general. That box represents the combinational logic time required to do the work in that stage, and we only save that work at the next rising clock edge. So we're… we take the entire cycle 5, to get our data wrapped back around to the inputs of the register file, and on that ending rising edge, that's when we actually save it. So if we're saving the result in R2 at the end of cycle 5, then we can take cycle 6 to actually read it back out of the register file. Okay. Okay. I had forgotten that we have one more slide of talking about The problem before we get to the solution. So, there's this additional issue that arises. The language that we're using to describe all these things is called hazards. There's a hazard. We cannot allow instruction execution to continue. We must detect the hazard. Because if we allow this to happen. We will read something out of the register file, but it will be the wrong thing, and we will do the wrong math in that second instruction, and then we will have an error. The hardware execution of the program will not match the software engineer's expectations. So We have to detect… that there's a problem. We have to detect that there's a data dependency. And the good news is that we have this control information that flows along with our instructions. Remember, the instruction decode information is flowing along through all the interstage buffers. This means that, for instance, I can keep track of what's address A, address B, and address C are for every instruction that's moving through my pipeline. So when I'm in the midst of executing my instructions here, during Cycle 3, I know that the address C for the add instruction matches Address A for the subtract instruction. I have these interstage buffers, and I can pull signals out of them back into control, and I can do a comparison. I can see address C for the ad matches address A for the sub, this is a problem, I must make the subtract instruction wait. Then in cycle 4, I can see that address C for the ad still matches address A for the sub, and I can make it wait. Eventually, in Cycle 6, we will see that there is no earlier instruction where address C is equal to R2, and at that point, we can safely read R2 from the register file and allow that instruction to proceed. The… One other issue that's not shown here. But it's important to keep in mind a pipeline. Is a well… is a well-regulated line. No cutting. So that means that if I have an instruction that is stuck in decode. I cannot proceed out of Fetch. For the next instruction. So, in Cycle 3, I'm attempting to fetch. I will successfully fetch the next instruction. And then I'll fetch it again in cycle 4, and I'll fetch it again in cycle 5, and finally I'll fetch it for the final time in cycle 6. Because it will be able to begin decoding in Cycle 7. Everything else behind the stalled instruction Asked to wait. Because you cannot have In simple pipelining, more than one instruction per pipeline stage. Spoiler alert, high-performance computers don't respect this anymore, but somehow still manage to make program execution match as if it did. But you can learn about that in 425. Ryan. You should take 425, Natasha. It's a great class. Ryan, yes? So… That's a great question, and I really appreciate it, because this is something that we should talk about at some point in the slides, and I don't know if we do. We're talking about capacitors, right? So there's no way for the values to not be replaced by something. We are not talking about something abstract. I mean, this is an abstract drawing, but the underlying hardware, it's just capacitors. So I have… something… Going through the pipeline behind the add instruction, but it's not the sub instruction. But it has to be something, and if I don't do it right, I could break stuff. So every time I have a delay of a cycle here, something else is advancing through the pipeline. Depending upon which computer architecture book you look in, it might be called a bubble, a pipeline bubble. And… what's… can you think of, like, a set of appropriate values to travel through the pipeline? Remember what's going, it's control signals and addresses and stuff like that. Any ideas? Well, if I… make sure that when I'm… when I delay an instruction, and something else has to move through the pipeline, as long as… I'm not actually writing to the register file, It doesn't really matter. So, you can change the control signals that are following along the ad instruction through the pipeline to be something that's non-destructive. In some processors, register 0 is the value 0, And so you could change the instruction to be adding 0 plus 0 and saving it into 0, which does absolutely nothing. But you wouldn't want the instruction that's flowing behind the sub, behind the ad in place of the sub to be, like, a load or something like that, or a store that would change processor state, that would be bad, we can't do it. So, in general, at the computer architecture cute diagram level, we talk about a no-op, a non-operation, something that doesn't change processor state. That's the thing that goes behind the ad as the subwaits. Ryan. So you'd have… you can only do the register zero thing in the case where register 0 is hardwired to zero. So for ARM, or for our processor here, we would have to choose some other non-destructive type of operation. you could… Or a register with itself. Or you could just have an arbitrary set of control signals that ensures that the register file is not updated. And then it doesn't really matter what else is going on, as long as it's not accessing memory. But the point is that it… We have values that have to go through the control stages, so the control registers, for each of the interstate buffers. There are values there, it's not undefined. Voltage is never undefined. So you have to pick something that's not gonna break anything. Does that answer your question, Ryan? You just have to pick one such that there is no possible dependency. And the way that you do that will vary from instruction set architecture to instruction set architecture. But I can't give you a more satisfying answer, unfortunately. I don't think ARM version 7 has a no-op, but it might have a way of saying, this instruction never executes. Okay. Oh, right. So we were just going to talk about that on this slide. Alright. So obviously, the whole point of pipelining is to not do this, because we want an instruction to finish every single cycle, and in this particular case, there's a really easy, straightforward solution, and it's called forwarding. The idea of forwarding is this. The add instruction, it's computing a value to go into R2. As it happens in our five-stage pipeline, it does so just as we need it. The subtract instruction doesn't actually need the result of R2 until cycle 4. Cycle 4 is when the subtract instruction goes to the compute stage. And it just so happens that the result is computed in cycle 3. So the answer is in the processor. Nope. the result of the add instruction is in RZ at the end of cycle 3, And in cycle 4, the subtract instruction can execute… can go into the execute stage. That's the earliest time it can go into the execute stage. So all I need is a way to get the output of RZ back into my ALU. That's called forwarding. Okay. And so… All we do… Is we add a path. from the output of RZ back to a multiplexer. for our source A operand. So that allows us to… read from… we will read… from R2, And we will read… let's see, I want to get back to the original code there. The subtract instruction… We'll read from R2 and R8. Whatever it reads from R2 is the wrong number. What it gets from R8 is the correct one. And then, when it is getting ready to do the actual computation, the real value of R2 will show up at the input of the MUX A control signal that detects this hazard We'll choose the appropriate output of that top MUX, And make sure that the ALU does the right math. And this is done by the hardware. The software writer need not know that anything happened. This is invisible to the soccer writer, Muhammad. What is the select? Whatever you would like to name it. We could call it, r-A-Mux, or A… A-L-U-A-Mux, or you could come up with a better name, yeah. much for that much. the instruction itself? Like, where it differs. So the… so, like, the IMUX bit, that comes from the instruction, right? But the… select for the top MUCs, the instruction doesn't encode that there is a dependency between the two, because the instruction doesn't know… the compiler doesn't know unless the compiler knows exactly what hardware you're compiling for, which is a thing. But we're assuming that that's not a thing in our particular case. The software doesn't know that there's a data dependency to be avoided. And so, the select signal there, it comes from this logic that is doing the comparison of the different source and destination addresses as instructions are moving. So the control… the control unit sees address C matches address A, therefore change the input to this MUX so that we get the RZ source. Instead of the output A from the register file source. That's separate logic that has to be implemented. So we can do… but it's the same logic that does hazard detection, it's just a little bit more complicated. If I can detect a hazard by comparing these different fields and the interstage buffers, then I can also take it one step farther and say, well, if this is happening, then just change the input to this MUX, and then you solve the problem. You have a question? Absolutely. Then we just add another input to the second mux. Yeah, right on. Well, so, the picture here is solving that problem. And then… yeah, I know, but the reason why the subcommand changed is because of what Yon Nun said. What if… What if… it's in RB, then we would just have to take… so we have a second path that we need. We need a second path from the output of RZ into the IMUX. So we have a wider IMUX with another input, Lucy. If I have clock cycles? If we need something. I agree. Yes. Yes, there are cases in which we cannot avoid a stall. And if we have a load followed by an ALU operation, because the load result is coming from the memory stage, we must stall at least one cycle. That's not avoidable, except by changing the order of execution of instructions. Which is, incidentally, something else that high-performance processors do. You write your assembly in a particular order. High-performance processors will say, I don't think I'm gonna do it that way. It'll be faster if I do it this way instead. And that's how high-performance processors can avoid stalls like that. But for our simple pipeline processor, a… Access following a load instruction. We'll always have… A data-dependent access following a load instruction will always have a one-cycle delay. It also requires additional forwarding paths. A forwarding path is any one of these lines that I draw from the output of a temporary register back to, generally, the ALU, but sometimes also to the memory stage. Okay, so here's another… Picture… So what if I have a different code sequence? Here, the sub is separated from the add by an independent instruction. The OR instruction is not doing anything related to the other two. But my sub still needs R2, and the ad is still producing it. So if we draw our diagram here. We still have to do forwarding, Because there's still, time that we would otherwise have to wait. You can see at the end of writeback, we're not forwarding to exit… we're not sort of naturally giving it to the decode stage. We need more instructions in between in order to be able to facilitate that. And so… we have to have another set of forwarding paths to deal with the case of, what if I have one instruction in between? In that case, I'm forwarding from RW back to the MUX. that goes into the ALU. So, can you imagine… A 20-stage pipeline processor that needs to forward results to the ALU from every single pipeline stage after the ALU execution. Just picture the multiplexers that you would need to be able to do that. by the way, the Pentium 4… Was a superscalar out-of-order processor, which means that it had more than one adder. So you could have something in a later pipeline stage, and it's going all over the place, potentially. All in the pursuit of performance. Just simple cases for you in 324, thankfully. But yes, if I have an instruction intervening, then I just need another forwarding path. And so, just like from RZ, there was a forwarding path to input A and input B. From RW, there's a forwarding path from input A and input B. That is… forwarding is a hardware solution. Two stalls? We could also take a software approach. A compile time approach. So, all of this logic that I have to implement to detect when there is a hazard. and all the extra wiring to perform forwarding? I mean, it's not free. Engineering is about trade-offs. If you wanted a really low-power processor, maybe you wouldn't want to have to detect hazards. You could lean on the compiler to solve that problem. The compiler knows If it knows what hardware it's executing on, that there's gonna be an issue. And so, you can have the compiler insert these no ops. Operations that perform no work. And then you just sort of separate instructions by enough distance. Or you go and find enough independent instructions to put them in there to fill up those spaces so that work is being done. It's not just fitting around. But there are entire families of processors that are designed in this way, because Part of the power problem for high-performance computing is all of the work that has to be done to try to find independent instructions that can be executed to avoid stalls. But if we just lean on the compiler. Then the hardware can be much simpler. Mentioned slow memory accesses being an issue. In this particular picture, we see the consequences of having a slow data access. Our fetches are proceeding just fine. Don't ask me why, it is just for the sake of drawing the picture. Maybe we have a separate, faster path to instruction memory than our data memory here. But if our data memory is slow, if it takes 3 cycles instead of just 1 to access data memory, then everything else behind that instruction, even if it's not a load, has to wait. Because nothing can advance past the memory access instruction in the pipeline. It's a well-regulated line, no cutting. So this gets to the issue that Lucy was raising. What happens if we perform a load? So, if I'm performing a load. And I'm saving the result of that load into R2, In this instruction, whatever is at the address specified by R3 is put into R2, and then R2 is used in the subsequent instruction, a subtract. I have to delay the subtract instruction, because I don't have the result of the load until the end of cycle 4. I can then perform forwarding, I can draw a line from RW to the ALU, I've already got it. for addressing this issue, I can use that same forwarding path. and then forward back to the ALU from the memory stage. The only solution to that stall is a software solution where the compiler finds an independent instruction to stick between the load and the subtract. But if there is no independent instruction that can be moved to there, then the CPU has to wait. You lose one cycle. It means that… In general, the ideal for a simple pipeline processor is one new instruction result every cycle. In this particular case, I would just have a cycle where I don't produce an instruction result. Yes. So we always stall in the decode stage. And the… The decode stage is where you can think of the decode stage as having the logic that's checking, can I get my operands? And so… The… we will stall in the decode stage in this particular case, because that's where we're supposed to read the register file, and then the next cycle, we're supposed to be doing computing, but we don't have the inputs to be able to do the computing, so we sit in decode until, when we advance to compute, all of our inputs will be available to us. Yes, Justin. There's a possibility, depending upon how you would decide to build it, What is an instruction? where… We need our operand not at compute, But at some other time. There's only one kind of instruction that we've talked about where this is the case. It is a memory access instruction. What's an instruction where it would be okay if my data didn't show up for me until the memory stage? A store instruction. So, a store instruction needs some of its operands for the compute stage to calculate the address. But the actual data that's supposed to be stored, it could arrive late. when the store instruction's at the memory stage. So if an earlier instruction is computing the thing that we're supposed to be storing. It can arrive… in time to… Hit the store instruction in the memory stage. And where you stall. My recollection of all the computer architecture I've ever looked at is that you stall things in decode, always and forever, but you could stall a store instruction in decode so that the memory… the data that it's supposed to store in the memory stage just happens to arrive on time. And the rationale for that, if I'm remembering my advanced computer architecture, is that once you free an instruction from decode, you just sort of assume that it's going to march along happily. But you intervene at the decode stage to delay things. However much you happen to need to. so that we can… the vitamin D. Nope. Yes. Like, the write enable for the interstage buffers is gonna be 1, unless there's a stall. And if you have a stall, that's where you have to sort of figure out, what do I do to the contents of that pipeline? What, actually, you would just write something different, possibly? You have to do something to Ryan's question of, like, how do you insert control signals in the interstage buffer that won't destroy things if I have a stall and a no-op has to proceed. So you might just… always be writing the interstage not always. I guess if you're stalling on memory, then you just need to have the… All the instructions behind it fixed in place. It depends on the instruction set architecture. And it's kind of, ultimately, a design choice. Go ahead. Yes. It's hardware, just like… The interstage buffer is RZ plus the control signals that need to continue on from it. It's RA and RB plus the control signals that are being sent along. Other questions? Okay. Control hazards. So we've done… data hazards. A control hazard is when We have branching involved. Because branching… Changes the sequence of instructions that we're supposed to execute? But in general, we don't know if a branch is going to be taken until several cycles after we have fetched the branch instruction. So… I'm going to fetch my Brad's instruction. And then in the next cycle, I will decode it and actually know that it's a branched instruction. Meanwhile, PC plus 4, I have fetched. And then in the third cycle, I'm going to actually compute whether or not I should be branching. Meanwhile, PC plus 4 has been fetched again. Two additional instructions have been fetched while I have been figuring out that I am branching, and that I might actually be doing it. Control hazards are a big deal. Because about 20%, again, of instructions are branch instructions. The amount of code that you execute before a branch? Not very much. Now, again, imagine you have a 20-stage pipeline. If you have a 20-stage pipeline, and 20% of instructions are branch instructions, you've got several branch instructions at different stages of execution. This is why, in the late 1900s, my daughter really presses me on this point. It's like. what did you do in the late 1900s? Like, oh, you've… you've lived in two centuries! Like, thanks. In the late 1900s, a lot of computer architecture research was done on branch prediction. how can I make a really good guess about which direction the branch is going before I know, so that I can fetch as many instructions on the correct path? while I'm waiting to verify that it's the correct path. Today, AMD, famously, in the Ryzen processors, uses machine learning to perform branch prediction. In general, branch prediction is quite complicated, uses up a whole bunch of transistors, because you basically look at which way did the branch go all the previous times that it went? And what other information do I have to help me guess whether or not it's going to do that again this time? Take 425. And you can go through all the painful details about how branch predictors work. Through here, we have a pictorial example. The top instruction is our branch instruction. We fetch, decode, and execute. Meanwhile, we fetch The subsequent instruction. And the one after that, instruction J plus 1 and Instruction J plus 2. Excuse me. But when I finally figure out that I'm going to be taking the branch in Cycle 3, I have to now just throw away… the work that I have done on instructions J plus 1 and J plus 2. If I'm taking the branch. Whatever instructions come after the branch instruction, I'm not executing, but I have started to do some work. I've started to fetch and decode and fetch. And then, just like before, when you would have a stall, You have to insert a no-op, a no operation, in place of those instructions, so that no work is actually done there. And then the correct instruction execution path resumes at, in the fourth cycle, in the fourth instruction there, Lucy. To add additional water. Our PC has been permitted. And… We're adding, like. Correct, but we don't need special hardware to do that, because it's always the same amount. In this particular example, and for the purposes of computer organization at McGill University in 2025, it's always PC plus 8 has already been done. Always and forever. That's the ARM version 7 standard. By the time we are in the execute stage of a branch instruction, we have incremented PC twice. So our relative address is relative to PC plus 8. Was there another… No, the no-op is not inserted by the programmer. No ops can't be inserted by the programmer if hardware is taking care of this, because the programmer doesn't know that it's going to be a problem. Well, so the programmer doesn't know how many no-ops to insert, necessarily. It sort of depend… like, in ARM version 7, the programmer would know that you have to insert But in x86… x86 goes back to the 80s, and it's still used today. You go from having no pipelining, to having 20-stage pipelining, to having whatever it is that Intel is doing right now. So there's no definition of how many no-ops you would have to insert. And so if you want to have a implementation-agnostic compilation, you're dependent upon hardware figuring it out. And the hardware knows, because the hardware knows how long it takes to compute whether or not the branch is being taken. And where it's taking it to? And so it can address the issues in the pipeline itself. Yes, and your name? Nicholas. Nicholas. The second… It could cause a problem. It could cause a problem if J+, if the instruction J plus 1 is able to modify processor state before we have figured out whether or not we're taking our branch. So… Again, computer engineering is not like the laws of physics. People have figured out how to deal with that. Take 425. But the… the easy answer to that question is, we just don't allow any instruction to advance the execution until previous branch instructions have been resolved. Because then, at that point, we either know to kill the instruction. Replace it with a no-op, or allow it to continue. But… In general, we don't wait until Cycle 3 to figure out whether or not we're going to take a branch. For this reason. And ARM version 7, doesn't actually do branch calculation in Step 3 at all. It doesn't do comparisons of registers, so we don't need to access the register file. And, it does its… it does the PC-related math, I think, probably in Stage 2. So there… there are… there are architectural strategies for eliminating this problem. And it's useful to do so because there are so many branch instructions, and otherwise you throw away so much performance. Does that answer your question? Okay. So this is, like, what we were talking about. So, in this picture, Our branch penalty is 2 cycles. When a branch is taken, we're throwing away the execution of two instructions. That assumes that we are using the ALU to calculate our new PC value. But if I just add an additional adder, then I can perform Address calculation in the decode stage. once I know that I have a branch instruction, and then my branch delay is a single instruction instead of two. I can… and if I don't have to do a comparison. then it's… then that's straightforward as well. But you can… you can make it so that you do the comparison in the decode stage. But remember, we were doing all that stuff in the AOU, because all that adder logic and stuff, it's expensive. And it uses energy. So there's a trade-off here. I can move that stuff for a branched instruction into the decode stage. At the cost of an extra adder, and at a cost of doing all the extra combinational logic for the comparisons. Add more hardware, but reclaim some performance. And that sort of… That is computer architecture history there. We want our processors to go faster, we want higher instruction throughput, that introduces new ways that things could have to be slowed down, and I can mitigate that by adding more hardware to try to stop the slowdown. Over and over and over again. How do transistors get used? That's exactly how. You use transistors to detect problems that are caused by trying to go fast and then fixing them. Okay… Alright. So that's… Oh, a hardware solution? But again, there's a software solution. Just like before, when we saw we could solve a data dependency by inserting no ops in between. Which allows the first instruction to write to the register file before the second one needs to read it. I did mention that we could just move instructions that are independent into those spaces. We can do the same thing to address branches. I don't know that there are many processors today that do this, but it used to be a popular option. What we do… is reorder instructions, If you look here. On the left, the branch instruction is the fourth instruction, okay? But over on the right, it's the second. If your computer works with a branch delay slot. What the instruction set architecture says is… In this case, the two instructions after every branch will always execute. Always. So, pick ones. that are independent from the branch, and put them there. So what we've done on the left. The branch depends on… registers R5 and R6. None of the instructions above it produce R5 or R6? The OR instruction uses R5 and R6, but it doesn't change them. So all of those instructions above the branch are independent of the branch. I can change the order of the branch instruction relative to those instructions without changing the output of my program. So I can take two instructions. in this case, the sub and the AND, and I can move them beneath the branch. The program has instructions in a different order, but it still computes the exact same thing. So in here, instead of having a situation where I would just kill the two intervening instructions in the case that a branch is taken. In a processor that uses a branch delay slot, you guarantee that they are always executed. And so you just pick some that could always be executed as if they were appearing before the branch in the program. Muhammad. Where do you go next? Well, so the branch is going to label 1, so it would just go down to label 1, but when we branch, we still execute the sub and the AND no matter what. So when the branch is taken, we execute the sub and the AND. When the branch is not taken, we execute the sub and the AND. Again, go through the steps. If you've already done this. You don't have to repeat the steps, because you're going to do them no matter what. No matter what… yes, Maeve. So what this does is it makes the code on the left and the right take the same amount of time. So the code on the right, if my processor uses a branch delay slot approach to addressing control hazards. It will always execute in the same amount of time. Whereas if I take the code on the left, And I have this approach. When the branch is taken, I will… Throw away two instructions worth of work. But when it's not, it'll be fine. Whereas with the branch delay slot. no matter what happens, I always execute the two instructions after the branch. So if it's not taken, I'm going to execute them anyway, but if it is taken, I'm also gonna execute them anyway. Does that answer your question? That's right. When I use a branch delay slot, and I can fill the branch delay slot with useful instructions, I never throw away work due to control hazards. just whatever comes next. PC equals PC plus 4. So… I was curious. And I asked ChatGPT about this, And… Honestly, I was a little shocked. So, I actually asked ChatGPT two questions from this lecture. The first one I thought was relatively straightforward. I presented the same code as we've been looking at earlier, and asked About, the pipeline hazards that exist. What pipeline hazards exist? What forwarding pads are needed? State the answer as briefly as possible. And it come back… it came back and said that there is a data dependency in R2 between the two instructions. And it specified the correct forwarding path that would be needed to mitigate that. Sorry? PhD level! So then… I gave it the problem that we just looked at. This is the exact code. We have the three independent instructions in the branch. And I said, Rewrite the sequence to eliminate pipeline stalls under the assumption of a two-cycle branch delay slot. Which is exactly what we just looked at. This is wrong. In a variety of ways. What is wrong with this code on the left? Yeah, exactly. Lucy. So there aren't dependencies. They fixed the wrong one. It's repeating instructions. And… the subtract here. So, the rationale that it gives for repeating the OR instruction is that that's a harmless repetition, which is true, except if you care about performance. It's making the code longer, And executing more instructions? Like, you get no points in my class if that's your solution to this problem. Zero. That is not an answer worthy of partial credit. But the second instruction that it inserts Then it copies it from before. And so we have R3 minus R8 into R3, and then we do R3 minus R8 into R3 again. This program gets different outputs than the original. I was astonished. Because you fail my computer organization midterm. If this is the way that you think through stuff, Yeah. No, I haven't. See, but then I would know how well it would do, and that would just make some of you feel bad by proxy. I… most of you will do better than ChatGPT could, just based on my experience asking the questions that I have. A lot of the questions that I have asked it are simpler than the ones I ask on the midterm, and ChatGPT has not done well. I would expect ChatGPT to be a B or a C student in this class, in general, based on… How lucky it is. Yes. That is… no, I don't curve midterms. Didn't I tell you? If you do better on the final than you did on the midterm, we just pretend the midterm never happened, so I don't curve the midterm. I curved the entire… Final grade. That's where the Caribbean happens. But last semester, I only added, like, one percentage point to everybody's grade, so it doesn't necessarily give you a lot. But I was astonished by this, because this is… It has the appearance of being plausibly correct while getting absolutely all of the reasoning wrong. No, rated incorrect and ill-advised. Don't do this. Where are we on time? Okay. Oops. Okay. Alright. Phil! That's it. If you have any questions, I will stick around for the next 5 minutes or however long. Next time, Instruction Set Architecture and Assembly Language Programming.

Seek back 10 seconds

Pause

Seek forward 30 seconds

Mute
Current Time 
0:28
/
Duration 
1:14:09
 
1x
Playback Rate

Captions

Fullscreen

Picture-in-Picture
