
2025-FALL / ECSE-324-001
Captions
Search current recording by keyword
Brett Meyer, Prof: I'm just gonna jump right into it, pick up where we left off. Hmm. So, we are in the middle of talking about memory access instructions. And we've come to this slide. where we formalized what I was trying to get at earlier. With the whole square… square bracket business? In your textbook, you'll see that they use a different notation for ARM version 4. ARM version 7 uses square brackets to indicate effective addresses. They are important because sometimes in the syntax for a load instruction. We'll have load, your destination register, we'll have something in the effective address calculation, and there might be stuff that comes after it. The stuff that's outside of the square brackets is not part of the effective address calculation. Affective address calculations again? They're done in the ALU, And they take the form of either adding a register with nothing, register indirect You can think of this, again, as RN plus zero, because the effective address is always calculated by the ALU. So just because you don't see a zero there doesn't mean that it's not there. The appropriate control signals for the register indirect instruction Is to have an immediate offset of zero. IMUX selects has to be equal to 1. Because we always have RM coming out of the register file, and when we're doing register indirect and immediate offset, those two types, we do not want to be adding what's coming out of the second output of the register file. Rn plus 0, or Rn plus a non-zero offset, which can be positive or negative. And… the way that this happens in this load instruction, and I can't remember if we talked about this, I think we might have. The load is really strange when it comes to operand 2. In all other cases that I can think of right now, which is to say there might be other cases that I'm not aware of. Plan 2, if it's a negative value, it will look like a negative value. The most significant bit will be a 1. It's actually… a maximum magnitude of 2 to the 11, and it can be a negative number. For load instructions. In the immediate field, it will always look like… it is always treated as a positive number, and sometimes we do subtraction, rather than adding a negative number. And that gives us a little bit more range That allows us to go plus or minus 4K instead of plus or minus 2K. But it also means that the load instruction, where the offset is positive, has a different opcode than the load instruction where the offset is negative. That's how the instruction communicates to the processor that we should be doing subtraction, rather than addition of a negative. And again. like, if you were… if I were to design an instruction set for you to implement on your own, I would not do this kind of stuff, because it just… you'd add it to the long list of rules that ARM decides to break when it's convenient to do so for the sake of performance. But this is an example of that. ARM is doing something Here, That it doesn't do with most of the rest of the instruction set architecture. Because it's advantageous to be able to go plus or minus 4K as opposed to plus or minus 2K. And apparently they had extra opcodes to spare. Are you tracking with me? Every other place, Operand 2, Will look like a negative number if it's a negative number, but for load. It's always a positive number, and sometimes we subtract it. Just remember that computer engineers are opportunists, always looking for little corners to cut to make things faster, more energy efficient, or whatever, and that involves breaking previously well-established rules. or creating exceptions. That makes your task as a student, trying to learn this stuff. more complicated, because you have to add it to another set of… it's yet another fact that you have to try to remember. But you should note that As far as details like this are concerned, if you ever need to know that the load that the opcode for a load instruction is slightly different, or whatever, I give you the ARM quick reference card, all four, six pages of it, or whatever, for your midterm and your final. You have that information available to you, you don't have to memorize it, and I don't expect you to memorize how instructions become machine code, or things like that. When it comes to the low-level details. Stuff like remembering that there's always a condition thing at the front and whatever, yes, I expect you to remember that, but I don't expect you to be an assembler. Yes, Muhammad. Whether it's positive or negative. It's headed by… Off toward another condition. Yes. So if we… How far back do we have to go? To get our instruction format. There it is. So what I'm saying here is that Operand 2, for the most part, is a signed number, except in the case of load and store instructions, where it is an unsigned number, and for a load or a store instruction, we just have two different opcodes, one where we're adding the offsets, and another where we're subtracting it. Which is unusual. I can't think of any other instruction that's like that. That doesn't mean that there isn't another instruction like that, but this is the only one that I've encountered so far that behaves that way. And the whole reason I'm pointing it out is to just continue to really emphasize this point that ARM makes RISC processors, except for when they don't. Arm abides by 2's complement representation of numbers, except for when they don't, and so on. And it's useful to have in mind that there are exceptions to these things. Okay, so… I believe we looked at our example here. Is that the first? Yeah. So, in this case, again, we're accessing from an array, ARR, ARR is an array of integers, meaning they each require 4 bytes. and we put the size of the element into R2, we multiply i by the size to get the offset for the element that we want. That's in… that goes back into R2 again. we add that with the base address of the array that's in R1, that goes into R3, and then we can load from the address that is stored in R3, plus 0, and we put the value that we want in R4. That's a lot of instructions to do. Just to make an access from an array. A pretty common thing. I bet that in your time at McGill, you haven't written many programs that don't have arrays in them. It's a very convenient way to represent data. So we don't really want to have all this overhead. So, we don't actually have to. So… An easy thing that we can do, right here, is, again. we take the number of, the size of each element and put that in R2, and then we can multiply i by R2 and save that in R2, and then we can use The sum of two registers. Our base address, R1, and an offset stored in R2, And we've just saved ourselves an instruction. The one that we removed is the add, where we add the base and the offset. Here, we're allowing the ALU to add the base and the offset for us. But we can do more. We can actually eliminate all of the instructions except for the load instruction. By optionally shifting R.I. So I, our index variable, is in R0, If we left two positions, We multiply it by 4, We add the shifted thing to R1, the base address. And now, suddenly, we've done all of that mathematical work to calculate our address, And then we just… We made our code 75% shorter. We eliminated 3 out of the 4 instructions. A lot more work is being done in address calculation. But this is faster, uses less code, uses less energy. Why does it use less energy To do the math like this. Then it does to do the math like that at the top. Less energy. Yes, what's your name? Say that again? I can't hear you. Hamza! Thank you. And what's your answer? Yes, that is definitely a piece of it. So Hamza said, we need to write things into fewer registers. Reading from SRAM and writing to SRAM takes energy. What's another source of savings? Awesome. Multiplication versus shifting. Yes, multiplication is much more expensive than shifting, because to do a 32-bit multiply. I essentially have, A whole bunch of adders that are all working in parallel. instead of having… if you think about a carry ripple adder, I have 32… Full adders to do a 32-bit carry ripple adder. To do a 32-bit multiply, I need 32 of those. Because I have 32 partial products. Multiplication is massively more expensive than shifting. So that's another way that I'm saving energy. What's another way I'm saving energy? We've talked about reading and writing the register file, but… ARDA. What cycles. So, you said one fetch at the end. And that's what I want to focus on. We do… we definitely do it all in one clock cycle, but it's a reasonable question to ask. If I'm talking about energy, Multiplication takes energy, which we just talked about, but an ad takes energy. And then the ad in the ALU is just doing the ad that we were doing in instructions before. So if an ad takes a certain amount of energy, I'm not really saving energy if I just do it later. But… Fewer memory accesses. I only fetch once. And your off-chip memory accesses, They tend to dominate your energy consumption. When you have dominate… they're a substantial part of your energy consumption. So here, I perform… for this… to execute this instruction at the bottom, the load, how many memory accesses do I perform? Phew! I fetch the instruction, and I execute it to perform the load. But for the above code, I'm performing 5. So, 60% of the energy used to access memory is gone. And not only that, Decoding an instruction takes energy. Going through the memory and the right-back stages takes energy. Actually, one of the first experiments that I did as a graduate student was looking at The energy… the energy variation between different instructions in an instruction set. And at most, on the processor I was looking at, which I can't remember what it was, Arduino did not exist. Raspberry Pi wasn't a thing. But the maximum variation between instructions was about 20%. Between doing nothing and doing a multiplication. 20% difference in energy. Why? Because you always have to fetch, you always have to decode, you always do memory, you always do write back. The only place where there's any variation at all is in the execute stage. So this is massively more energy efficient than that. Because we eliminate all of the other overhead related to doing those instructions. Any questions about this? Yeah, San Diego. No. You don't have to specify the zero. It's implied if you don't do it. The assembler knows that you mean plus zero if you don't put the zero in there, and it'll set up the instruction to be plus a zero offset. If you look at the binary, you'll actually see that operand 2 will be 0 in that particular case. Alright, so we can do the same thing with stores. Let me just show you the end result there. Base address plus offset, which is calculated by taking i and shifting it. We shift by 2 again because our data type is an integer, which requires 4 bytes. In your Lab 2, I very intentionally decided to not have you work with integers. You work with half words and bytes. Which means that you won't be shifting by 2, Sometimes you won't be shifting at all, other times you'll be shifting by one, you'll have to go and figure it out. I could just as easily say, what if your data type has… 8 bytes! Well, then you need to ship by 3 instead of 2. All depends on how big your data type is. Okay. I am not going to go through this example, but there's a bunch of different effective address calculations you can do, just to check to make sure that you have understood what's going on. Now we're gonna talk about pointers. The reason we're going to talk about pointers isn't because pointers are fun. Which, I would say, in general, they are not. I was part of the beginning of the experiment teaching Java in universities. So when I was 18, my intro programming class as a freshman was in Java for the first time ever at the University of Wisconsin, and… It was great, because we didn't have to learn pointers, and then I took operating systems, which was taught entirely in C++, And I was grateful to have attached myself to someone who had learned how to program initially in C++, because they understood pointers. I did not come to understand pointers until I was a graduate student, and the simulation software I was contributing to was written in C, and so I had to. I didn't really learn it in operating systems. Because I had Kahn. He, like… He understood that stuff. No problem. We're gonna be talking about pointers, because… It's really useful to think about pointers in terms of thinking about how memory accesses work. Because pointers… the purpose of a pointer is it's a variable that stores a memory address. Just like we use registers to store memory addresses. So, we have some C code. Where the first thing that we do is declare a pointer to an integer, and then the next variable is an integer, and then I've got an array of integers. On the fourth line, we set our pointer P equal to the address, of A, at Elements, Fourth. Zero. 1, 2… Three. When our index is 3, we're accessing element number 4. And then the question is, if I set X to the dereferencing of P, what value is in X? And the answer is… 42! And because what this… what this assignment is doing is it's loading from the address pointed to by P, and putting the value in X. Just like we would load from a register. So this is what's happening here. We have to first get the value of P, into a register. In our assembly program, P is a variable, it is declared someplace. The assembler knows that P is declared. It can… your assembler can use names like this. And what the assembler does is it figures out where is P in memory, and then it puts the address calculation in place of the name P there. So we load from the memory location where P is. into R0, that gives us the address stored in it. It's the value of the variable P. We're loading the value of P into R0, then we load from the address. that is in R0, that's the second instruction, into R1, And then we can store the value that is in R1 into the memory location where X is. That's how we implement that one line of C code. Ed. Why can't we just put P directly into square brackets? So… So, the assembler knows where P is. But it does not know what the value of P is, because it can change over the course of the execution of the program. So we have to load from memory the variable P. And then we can put that value in square brackets by Like, what we did for the second instruction. Does that answer your question? Because P isn't a constant. If P were a constant. We could do that. And the… there's a way of telling the assembler to do that for constants, but P is a variable. Does that make sense? Muhammad. The square brackets around R0 is, Just demarking the effective address calculation. The dereference is kind of… two pieces, right? We have to get the address that we want to load from, and then get the data from that address. So the dereferencing is really the… it takes two instructions. Two memory accesses to dereference a pointer. One, I have to get the value of the pointer. Two, I have to use that value to get the data I care about from memory. Okay. So this is, I don't know that I ever actually thought about this until I taught this class. We assign data types to pointers. This is… so this is a part of the course where we really benefit from the fact that Professor Dubak is a compiler's expert. That's what he does his research on. And these days, that means fancy compilation for machine learning stuff. It's really quite cool. But I just never considered the implications of what we're gonna see in the next few slides. We say it's an integer pointer. And that allows us to do arithmetic on pointers in a meaningful way. So, if our pointer value is hex 10000, when I do P plus 1, it does not change P to be hex 1001. What does it change it to? Yeah, Harda. 1004, why? Leo, Theo. Because it goes to the next integer. And in this case, it is a full word, but you have a pointer to integers, just like you can have a pointer to a struct, and it goes to the first byte representing the next possible location of a data type of that size. If we had a character pointer, incrementing it would just point you to the next byte. If we had a half-word Pointer, incremented would add 2 to the address. I'm also… Not going to go through all of this. But this code does compile. And it is interesting. I'm gonna highlight a couple different things in here, just so that we get the syntax right. So we start off? By setting the value of our pointer PTR to be equal to the second element in our array. So when we print the pointer, we print the address, because PTR is a variable that holds addresses. The first print is going to be 1004. The second print dereferences the pointer, that's gonna… that's gonna print the value that's there. 26. The next one, we print pointer plus 2. Pointer is the variable, it points to integers, plus 2 means give me the address 2 integers away. So that's 100C in this case. Whereas dereferencing it is going to give us 45. What happens with the PTR++? What's the value being sent To the print function in that case. It's, yeah, it's like PTR plus 1, which in this case is 1008. Because PTR is 1004. But then when we print PTR, it's still 1004, because PTR++ Oh, sorry, it's actually going to be H in this case, because PTR++ does change it, right? Let me back up a step. PTR++ uses it, then changes it. I'm talking myself through this so as to not make a printing error. But you should go… you can grab this code, and then put it in a C program, and then just run it, and you can see this for yourself. So PTR was 1004. We would do PTR++, we will print 1004 and then increment it to 1008, and that's the second thing that we'll print there. The next print statement… It adds before using. plus plus PTR, you increment before you use it. And then down below, we have dereferencing on those different versions. All of these different variations of pointer arithmetic are useful in assembly language programming, because ARM has load instruction templates that match these use cases. Where you access A base address plus something. You access a base address plus something, and then change the register that you're using to do the access. You can add before you load, you can add after you load. Everything that you see here, there's a different load instruction variant for. And this is something else that I didn't know until I taught this class with Professor Dubak. In case I haven't said it before, you know, this class is offered every semester, right? Professor Dubak and I tend to alternate semesters, where he'll teach it one term, and then I'll teach it the next, and we work together on slides and assignments and stuff. It's a great working relationship, and I'm grateful for him. And… I was however many days old when I learned that you might write code with for loops, and your compiler will change it into while loops, because then we can use pointers. Which are more efficient. So if I do a natural translation of the for loop, Then what we get… is the stuff that we've been talking about before, where i is in R0, we shift it, we can then add it to our base address, perform our load, and then we increment I at the end for the next loop iteration. Very standard. What the compiler will do instead, it'll see your for loop, and it'll turn it into a while loop, and use pointers. The code… the programs are equivalent. But the advantage over here is is that if I have a pointer that I change after I use, Then that encapsulates the addition. And I don't have to use as many registers in this code. Same number of instructions. But I'm not using R2 anymore. Because I'm not keeping a base address separately anymore. And, like, I think it was Lucy asks, last time around. What happens if you run out of registers? Well, then you have to start adding additional memory access instructions to your program. Which makes your code longer, it makes your code slower, it makes your code use more energy. So using fewer registers is a win all around. Muhammad. Yes. Because the variable ARR itself is actually a pointer. Ryan. Well, so… Here on the left, we're keeping two registers, essentially. We're keeping the base register, and we're keeping I. And so I have to add the base register, plus I times something, which gives me my offset. Base plus offset gives me my address, and that's what I load from. But over here on the right, we don't have a base address anymore. We take our pointer, we set it to be the base address at the beginning, outside of the loop, but after that, we're just adding 4 to it. So, it uses the same number of instructions. But it uses one less register because I'm no longer keeping the separate base register plus an offset. Santiago. Yes, so in the C code. Since we defined the data type of our pointer, the compiler knows that when you do plus plus, you need to add plus 4. But your assembler? Doesn't know anything about data types. Your assembler doesn't know unsigned versus signed. You have to tell it that the data is signed or unsigned. You have to tell it that the variable is an integer or… or whatever. It will do exactly what you say. The compiler, what the compiler does, because the compiler is a very general-purpose program, It will… It will call a function that we talked about last time, sizeOf, Because if I compile… this code on a 16-bit ARM processor. The size of an integer is 16 bits. But if I compile it on a 32-bit ARM processor, the size of an integer is 32 bits. If I compile it on a 64-bit ARM processor, the size of an integer is 64 bits. The code will work replaced. But the odd there will be different in each of those cases, because if it's just a 16-bit integer, plus 2. The 64-bit integer, plus 8. The compiler uses standard functions for all data types in order to figure this stuff out. Compilers are amazing, and they almost feel like magic, but they're really just very systematic about translating your code into a graph, and then from a graph into machine code. Okay. So… let's see… For example, and here we have our first case where we have square brackets and something outside of it. If what we want is to implement the V equals dereference PTR++, The load instruction that does that, all by itself. including the ad that comes after, is right there. Load R1, R0 in square brackets, showing you the effective address calculation. Comma 4 means… after I use… after I calculate my effective address, Add 4 to R0. So we use R0 as your effective address by itself. But once that's done, we add 4. But I can get rid of my add instruction. But this isn't really a risk instruction anymore. And we can't do the simple register file that you've got in Lab 1 anymore. Because I'm saving two different values to the register file when this instruction executes. I'm saving an updated R0, And I'm saving data into R1. I have 2 right ports. Into my register file. And two write addresses into my register file, instead of just the one that you've seen so far. We can do this in another way. Slightly different, just depending upon what the initialization of R0 is. Here, we put the 4 inside of the square brackets, meaning R0 plus 4 is the effective address calculation, and the exclamation mark at the end means save the new address into R0. So R4, in each case, equals R4 plus 4, but here, we use it to access memory before we do the addition. There, we use it to access memory after we do the addition. I don't really know how this works in the data path, because it also kind of doesn't. In this particular case, the plus 4 has to happen somewhere, but we also have to just send R0 to the memory interface. Whereas in your Lab 1, what goes to the memory interface is the output of RZ, But isn't RZ gonna be our 0 plus 4 here? there must be some other way that they save the original value of R0 before it goes into the ALU, pass it to another temporary register, and that's what goes to the memory interface? But again, the instruction set architecture doesn't tell you how they implement it, only the features that are available. And then we're left trying to make reasonable assumptions about how it might actually work in hardware. So, I think I've said this a few times, but I just want to say it again. Arm? Especially, it's concerned about this bottom bullet. It reduces system cost to reduce code size. Because it means that my memory, where I store my program, can be smaller, The chips that go into… Toothbrushes, belt buckles, dishwashers are not super expensive. The processors themselves, they might be a couple bucks. And the memories aren't themselves super expensive either, but if you can cut the memory that you need by 25%, Or by half Then that can translate to a relative savings of your electronics that's substantial, And… ARM does an awful lot to make it so that we can express programs very succinctly. Which literally saves money. Not something that we often think about when… you go and download a game on Steam, and it's like a gigabyte, and you're like. What's a gigabyte among friends? It does hurt a little bit when the games are 40 gigabytes. But it used to be that… You know, pixel art kinds of games were not a gigabyte. the cost of memory doesn't matter quite so much for our laptops and iPads. But it definitely does for embedded devices that we're trying to sell for $5 to $10 a piece. Oh! Okay… boop. All right. So here is a relatively complete picture of what it looks like to access memory in ARM. These are all the different variations that are possible. The exclamation mark means… we update… RN… With the new value that was used to act… with the effective address that was used to access memory. And when we have stuff outside of the square brackets, it means we use that to update RN, but only after the access has been performed. I have asked in the past exam questions where I say. I give you a handful of load instructions, and I say, which one of these? Can or cannot be executed on this data path. And it's a matter of understanding Where the information has to go, and where it's coming from, in order to implement the work that is specified by the instruction. Okay. We are almost done talking about load instructions and store instructions. So far, We've been talking entirely about LDR and STR. Those instructions manipulate words, 4 bites. There is a suite of other instructions available to you that you will be acquainted with in Lab 2. That access half words, or bytes. We have LDRB and LDRH for loading half-words and bytes. And we have LDRSB and LDRSH, For sign extension access of bytes and half words. Two questions. Why do we have LDRSB in addition to LDRB? Why do we need a load instruction that sine extends when we access a byte? Ed. Well, so that's an interesting take. So if I… allow me to restate what you said. If I know that my number is unsigned, then I don't have to do sign extension, I can just pad with zeros. Which implies that if I know my number is signed, I would want to do LDRSB, so I can pad with zeros or ones. And I guess the question I was trying to ask is that why do we have to pad at all? Santiago. It's true, when we… when we access memory in ARM, we get a full 32 bits, but then the processor's only going to get a part of that. Whenever we load a data type from memory that is smaller than our processor word, we have to tell the CPU what to do about the extra space. Because our internal registers are 32 bits, if I'm only loading a byte, I have to be explicit. What happens to the rest? And so, if… As the programmer, or as the compiler, you know that the number that you're loading from memory is signed. which means it might be negative, then you must use the SB variant to ensure that a negative number in memory as a byte Stays negative when it becomes a 32-bit number. Otherwise, it's just going to become a large, unsigned 8-bit number, which will break your code. Second question. Why do I not? have SB and SH variants for store instructions, Ed. I like the way that you're thinking. But when I store a byte, I'm only literally storing 8 bits. Maeve. Yeah, I feel like we're… getting very close to a concisely stated answer. Anyone else wants to add a thought? Because you're right, we don't want to waste that extra space if all we're storing is a byte, Dragos. Like, in memory, let's say if you have… if you're just going by, you might have basically four values in there. So if you just put everything in there, you're gonna lose all their accounts. It's true, if we're storing a byte, we definitely don't want to store everything else. The reason why we don't have sign extension on the store side, though, is that we don't need it. It's completely unnecessary. Because if I have a negative number that's an 8-bit number, the bit 7, if I go down to 0, bit 7 is gonna be a 1 to indicate that it's negative. And when I store those 8 bits, the number that's in memory is gonna be negative. I'm going from a larger representation to a smaller one, Sign extension is irrelevant. It is only when I go from a smaller representation to a larger one that sinus tension is mandatory. Otherwise, I'm breaking my data representation. There's no S variants for STRB or STRH. All right. Loading and storing multiple words. This is another not-risk kind of thing that ARM allows us to do. It is also a departure from pipeline-friendly design. And for the most part. We've talked about how instructions either require one memory access to execute, like an add requires one memory access, or two memory accesses, like a load or a store. You have to fetch, and then you actually perform the memory operation. But load multiple and store multiple. can perform… Many memory accesses over the course of their execution. So, I'm gonna flip… no, okay. Just look down here at the bottom, where we have LDM, IA, R8, R0, R2, R9. What this instruction specifies… is that… We want to… We want to take the address, 8. And using that as the starting point. We want to load 4 consecutive words. into R0, R2, and R9. LDMIA starts with R8, and loads that address into R0. R8 plus 4 goes into R2. Our 8 plus 8 goes into our 9. This ends up being super useful once we start talking about the stack. And pushing and popping from the stack to facilitate function calls. Because… This vector here at the end can have as many registers as I want in it. It can, in fact, have all of them… I can load into all of the registers, if I want to, using this instruction. All 16 of them. One thing to note now, which you will forget, and then if you do… You will be very frustrated as you program in Lab 2. It does not matter what order you put them in in here. Because of the way that the instruction is implemented. R0R209 has the exact same behavior as R9R2R0, R2R9R0. The order that you specify does not specify the order in which they get loaded into. You're just giving items in a list. And if we perform a load. Registers are accessed in order from the smallest to largest index. Whereas if we provide… if we do it in a store. It's from the largest to smallest. Again, the reason why this happens this way will become obvious once we start talking about pushing and popping, probably on Wednesday. LDM also takes a suffix. In this case, there's IA, which is increments after the transfer. PTR plus plus. But plus plus PTR is also supported. You can increment before the transfer. PTR minus minus? Is decrement after the transfer. Minus, minus. PTR is decrement before the transfer. Again, all of your different ways of manipulating pointers are supported by this instruction. Muhammad. The order doesn't matter because ARM says it doesn't. And that has to do with the fact that If we think about… let's take an information theoretic approach to considering this. I can specify any subset of all 16 registers inside the curly braces. If I needed to encode the order that all of those registers appeared in my list, I can't do that with a 32-bit instruction. LDMIA is a 32-bit instruction. What it does… is it uses 16 bits somewhere in the machine code as a bit vector to say, do I do R1 or not? Do I do R5 or not? The order in which you specify these things doesn't matter. If a register appears in the list, we twiddle the bit in the machine code, and it gets ticked off and done in the order specified by the instruction, either LDM or STM. But we can't actually encode ordering, because we only have 32 bits. Yes. I have no idea, but I bet it would not assemble. Or, if it did assemble, it would just do it the one time. That's an interesting question, though, I've never… I think that's the first time I've ever been asked that. Rachel? I'm sorry, I think I might have missed it, but… So, we're supposed to have R8 upstellation Yes. Always? Yes. It's like the base address for the transfer. And then whether or not it's actually the address that's used for the first element will depend on whether or not we're doing increment before, or decrement before, because then you're like. Adding 4 to the base before you use it or not. But LDMIA is… perhaps the easiest to conceive of, because we're doing PTR++ in that case. Muhammad. Yes, the values at R8, and the value's at R8 plus 4, yeah. And the machine code are installed later than R0? So… Later. Yes, because it goes through the list. From smallest index to largest index, adding 4 each time. I don't know what happens if you put R8 in this list either. That's another little experiment that could be done. You can put PC in this list. There are actual legitimate use cases where you would put PC in this list. We will see in… not today, I don't think, but… Perhaps next time. So… How many… Memory accesses are required To execute this instruction at the bottom. We've got one vote for four. Two votes for four. We have to fetch the instruction. we have to fill R0, we fill R2, and we fill R9, and that requires a total of 4 memory accesses, because the ARM ISA says that we get 32 bits from memory. Memory provides 32 bits per access, not more than that. It is not possible. to do this in fewer than 4 memory accesses? Yes. That's a good question. Where do we store the offsets? So… In terms of thinking about, like, As an immediate value, It's a good question. We probably don't. Because we have to use operand 2 plus 4 more bits, just to be able to encode our 16-bit vector, but the opcode for LDM is going to be different than the opcode for LDR. And so the opc code can signal to the control logic that that input to the ALU for effective address calculation should be 4. I suspect that's how they do it. But again, state secrets and all. unless you are willing to pay enough to have a RTL-level IP block implementation you wouldn't actually ever see. You can include, if you want to do a design, let's say you have an idea for a cool widget, you can include an ARM processor. The lowest level of licensing, they essentially just give you a literal black box The contents of which are encrypted, and you can drop it into your design, you can do all your digital system simulation or whatever, have no visibility into it whatsoever, and therefore be unable to answer this question, unless you're willing to pay enough that they open up the guts for you. I imagine this is the deal that Apple has with them, because Apple has its own specialized version of ARM processors. They probably get the guts opened up so they can go and make changes and stuff. So there are people at Apple that know the answer to your question, but I don't. Okay, so 4 memory accesses. If we assume a three-stage pipeline, fetch, decode, executes, which is the underlying assumption for ARMv7, how many clock cycles does it take this instruction to execute? I asked this question of you now, because I've asked this question in 444 before, and been very disappointed at how little people remember from 324, so I'm trying to make sure that you get to 444 knowing how this stuff works. Ed. Why 12? Yes? I love your thinking, but it's only worth half credit. That was… that was… you're… you're… you rationalized in a reasonable way. But that's not the right answer. Because I don't have to repeat the fetch stage. I fetch. ID code, And then what happens? Execute, execute, execute. I just stay in the execute stage, doing all three data accesses. And so this is a kind of stall that we didn't talk about when we were talking about pipelining. Because… This is an instruction that just hangs out in the execute stage for a little extra time. Not because it can't advance, but because that's how it gets its work done. Meanwhile, everything else behind it in the pipeline is stuck. But if we have a… If we have an LDMA, LDMIA instruction that accesses 12 registers, then it's fetch, decode. execute, execute, execute, execute, and so on. It just sits there in the execute stage for a very long time. So in this example, 5 cycles. 4 memory accesses total. 5 cycles of execution. Because we don't have to repeat decode, and we don't have to repeat fetch. Yes, Yong-noon. Yes? Did I butcher it? Yeah, it's okay. Sorry about that. Data again. backwards. It's using R8 as the pointer. And then it takes the value at R8, it goes into R0, at R8 plus 4, it goes into R2, and at R8 plus 8, it goes into R9. So, kind of like… that. So, as a memory access instruction, LDM also allows us to use the exclamation point, meaning we update the register that has the address after everything is done. Something else that you can do is that you can use the little dash notation, that R6-R8 just means R6, R7, R8. That just makes it easier to specify in assembly. There's no difference between that and specifying R6 comma, R7, R8. Rachel. Yes. Yes, 5 cycles of execution for that. Because we fetch, we decode, we execute for R0, we execute for R2, we execute for R9. Assuming a three-stage pipeline, there is no memory and write-back stage. Correct. Sorry. That was an imprecise way to state it. It takes 5 clock cycles for this instruction to complete. Three of which are in the execute stage. Muhammad. Construction and… Our processor level. you cannot perform this instruction in your processor in Lab 1. Your processor in Lab 1 has no way to just hang out in the memory stage, and it also has no way to update multiple registers in the register file. So you cannot do this. ARM version 8 does not do this, because it is not pipeline friendly. ARM version 7 is targeting more embedded systems devices, toothbrushes and dishwashers. ARM version 8 is what is in the laptops and iPads in this room, if they have ARM processors, the more high-performance option. There is no LDM in ARM version 8. Because it does not pipeline well, which means if I have a 10-stage pipeline processor design. I don't want to do this. I would rather have… 10 load instructions than an LDM that accesses 10 registers. Because for high performance, I'm willing to pay for the extra memory. Okay. Last topic in memory accesses. And this ends up being an important one for Lab 2 and for the exams, because I always ask questions about this. We go all the way back… where was it? Further. Okay, here. LDRR0P. the assembler, just like you declare variables in C, or in Java, or whatever. You have to declare variables in, in assembly. And you can name them, which is convenient. So, in this code here, we would have a variable P named somewhere. That's our pointer variable. And… We're performing an LDR, from the variable named P into R0. There's an effective address calculation that's being performed there that is invisible to you as the assembly program writer. That effective address calculation? is… Performing PC relative addressing. PC being your program counter again. What PC Relative Addressing does is it uses the program counter as the base address. And then an immediate value is the offset. So, if, for instance, I have… The contents of memory over there on the right. And under the assumption that I have an ARM version 7 processor, which, by the time any instruction gets to the execute stage, the program counter has been incremented twice. Fetch, decode, execute. So when we're at the execute stage for this load instruction, PC is now… Plus 8, relative to the location of this instruction. The location of that instruction is 1000 in memory. At the time that the load is executed, PC is 1008, So, PC… Minus 16 will take us from 8… To 4, to 0. to FFC. So what this instruction does is it puts the number 26 into R0. If I did LDR PC minus… 12? I would just be loading that instruction Right? Or am I doing that wrong? Because if we're at 8, Yeah, it's FF8. I was off by 1. Gotta beware of off-by-one errors. Let's… let's do that again. So we're in… When we are executing the load instruction, PC is equal to 1008, I want to do minus 16, so minus 4 would give me 2, 4, minus 8 would give me 2, 0, minus 12 would give me FFC, minus 16 would be FF8. So the number in the… in R0 is 78. Ed. Let's see how far I have to go. Yeah, so the 78 is data. In this particular case. So if we have… We will get to this eventually, but in this example. It used to be when you were writing C code, you were required to define all of your variables before you had any operations. You had to write your code so that all your variables were defined like that before you had any code in it at all. You don't have to do that anymore. The compiler changes your code, so that's what it looks like, but you don't have to. And what… the way this translates into assembly is you've got data defined above where your program starts. Which means that, if I want to access this data that is defined above my program. PC relative addressing is actually a really convenient way to do that. Let's see… Because… In that particular example, the data is pretty close to where the software is, It's within 4K bytes. Remember, my operand 2 can give me minus 4K to plus 4K. If I'm using my program counter, as long as my function isn't too long, I can put constants and other stuff above it or below it, and access it using the program counter. Which is literally how this is used. When you define variables in your software for Lab 2, PC Relative Addressing will be used to access it. Because the data is close enough To your software that it can. If your data were not close enough, we would have to do more work To access your data, because if we need a full 32-bit address. Then I have to perform an additional load instruction, or two move instructions, in order to create that address to use. Whereas the assembler knows that if I want the number 78, and I'm executing at 10000, that the correct offset for it is minus 16. Because the assembler knows where your array is. And it knows where that instruction is. Ed. The assembler figures it out. You won't have to write an instruction that looks like this on your own. If you name stuff, And refer to it by name. And don't try to start accessing things in the middle of arrays. Because if that's an array there, and we want code that is going to walk over every element, you would just specify the base address of it. And then you would have some math to go to the next one and whatever. And you wouldn't specify the base address in hex. Because that would be insane. Instead, you'll name your data something, and then you'll just give the name, like P or X. Awesome. Foreign values are part of a career. 26 would be at the last one, because the array starts from a low address, FF0, and then… Goes to higher address. You have to go further. Cheers. You have to go further to get the first one. You'd have to go… instead of minus 16, you'd have to go to minus 20, minus 24 to get to the first element. No, none. They didn't get quiet. And, just opening up the address of that. There you go. Yeah. Okay. So if I have my data, Defined right above my program. So, let's see, we have… 96 minus 8… 70… 8, 26… And then I have LDR, R0… PC… Minus 16. I can do PC relative addressing, because in memory, this looks like that. Oftentimes in assembly, like, literally in 3 slides, I'll introduce the idea of assembler directives. Directives are not instructions, but they tell the assembler what it is that you're doing. One directive that you can give is to specify .text. text tells the assembler, this is code. And it will put it in the place where code goes in your processor. Because we don't want, especially in ARM processors. We generally will put code in flash memory. Why do we put code in flash memory for typical embedded systems? Because when I turn my thing off, Flash memory holds its value. iOS on your phone, or Android operating system, it's stored in Flash, which means you can turn off your phone, and then turn it back on, and all the data's still there. When I was growing up, flash did not exist. Anything that had permanent storage had a hard disk drive, metallic spinning plates. The first iPod also had one of these. But all of the devices that we carry around now, even our laptops. They don't have hard disk drives anymore in general. They have flash. I put my code in Flash, because my code is constant. It must always be there. Whereas my data will come and go as I execute. So if I do, I can specify .text for that, and I can specify .data And then have my .word… 96, minus 8, 78, 26, and then the assembly would look at my code and say.data, this is going someplace else. Let's assign this to… Random access memory. And then this goes in flash. You can do this in your emulator, it is a bad idea, you will… it will not assemble correctly without additional information, because the assembler then doesn't know what address to use to put this in. And what if… Text still goes to… this… That's data! those two… That. That's a 32-bit address. Is the difference between these two addresses less than 4K? It isn't. This is a much bigger number than this one. And if I… actually, if I just change this into just F, then I can't even use a clever trick of wrapping around. This is a very big number, this is a very small number, I cannot add or subtract up to 4K to this and get to that, which means that I have to save this address someplace else. Incidentally, it would save next to my text, I would have to save the value F 000000, and then I would have to insert a new PC relative addressing to load the address into a register, and then I could perform my memory access against the data. When my data is far away. It's like having to do a pointer dereference. But if I can keep my data close, I can use PC relative addressing to do it. And if you want to see the example just fully written out. My goal is to be able to do something like LDR into R0 of a particular value, but what I would have to do first is LDR BC… Let's see… R0… PC… Minus 4. No, maybe just PC. Do… do… yeah. And maybe the data that's here is… So if my data is far away, I again use PC relative addressing, I want PC which is PC plus 8, That gives me this address into this register. And if what I want is the number 78, then my effective address calculation is… that. If my data is far away, this is equivalent to that. This is a lot more efficient. So, in general, on the exams, I will not ask you to write a program? But I will ask you to fill in the blanks in the program that I give you. Because doing this stuff on the chalkboard is actually really hard, because there's so many details to remember. So I'll give you, you know, a 20-line program. And then I'll erase kind of the different things. And then you just have to fill those in. But this is why… this is why we keep our data close. There are advantages to keeping our data close, because this uses twice the memory accesses, twice the memory access energy, twice the memory access latency. It's also a lot bigger, because instead of having just the load and the data, I have two loads, and I have to still save the address someplace else. Are you tracking with me? Some yes, some no. We will continue to see examples of this as we go along. You will run into this in your Lab 2 programming, and you will see me ask questions about this stuff on past exams. But you have… the reason why we talk about the mechanics of memory accesses before we get into writing these programs is so that you understand the implications of the choices that you make when you write programs. If I want to do LDRP, LGRP only works if P is close by. If you put… if you tried to put this in with just data and text. It won't work because the assembler doesn't know where P is supposed to be unless you tell it. Muhammad. Oh, I have, we need R1 or something there. It could still be R0 to make it equivalent. Button. Yeah, and this would be 04, and this would be 08. Bye there. Yeah, our PC is there, and what I'm trying to load is this number into R0. So the first instruction is just getting the address of the base address of my array, and then the second instruction is actually loading this number, whereas this instruction all by itself gives us that same number. At the end of this instruction, R0 has 78. At the end of this instruction, R0 also has 78. Ryan. So the assembler knows. Yeah, exactly. So the assembler will see The assembler will see that you're trying to access data that is at a known location far away, so it will save the address somewhere below the end of your program, or below the end of a function, and then it knows how far away it is from the instructions that are trying to access it. Because the assembler knows the memory location of everything that you have done. It's actually… the beautiful time-saving thing about assemblers is that you don't have to keep track of memory addresses. It does it for you, and so it knows, if it puts it here, it knows that PC is the address at that moment in time to be able to get it. William. Oh, that's a problem. Yeah, I mean, this is, like, garbage, so they probably have an error. Yeah. Depends on how your processor is implemented, exception handling. But chances are, it would say, this is not a valid opcode. And then it dies. It halts. When you do Lab 2, There is no halt instruction in ARM version 7, so in Lab 2, you will finish your programs, each of them with an infinite loop. You just branched to a stop label over and over and over again. There's no halt instruction. And… Every memory address is just like any other to the processor. It doesn't know when your program's done. It just grinds along. Happily. Forever. So yeah, that won't work. And the first couple of programs that we would look at in the next few slides, I don't think we're going to go further than this today, but the first couple of programs that we'll look at, if they don't have an infinite loop at the end, then yeah, you get down into memory, and it just… problems. Doesn't… doesn't do… doesn't do anything useful, might do things that are not useful, decidedly. Ed? The previous one? Not that one. That one. Why is it PC? So, it's in ARM version 7, the PC always points plus H relative to the instruction. High definition, because the assumption is a three-stage pipeline, fetch, decode, execute, and it's a pipeline, so after we fetch, plus 4. After that same instruction is decoded, plus 4. So then you're in the execute stage, two other instructions have been fetched, PC is pointing 8 bytes farther than it was when that instruction was fetched. Yes. That's right. There's too much… too much chatter in here, so we'll just call it a day, and if you have additional questions, come see me now. I will see you on Wednesday.

Seek back 10 seconds

Pause

Seek forward 30 seconds

Mute
Current Time 
0:23
/
Duration 
1:18:40
 
1x
Playback Rate

Captions

Fullscreen

Picture-in-Picture
